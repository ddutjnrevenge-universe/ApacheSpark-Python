{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a60a0ade-cd36-40c4-9f7e-6211108de6e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Bronze():\n",
    "    def __init__(self):\n",
    "        self.base_data_dir = \"/FileStore/data_spark_streaming\"\n",
    "\n",
    "    def getSchema(self):\n",
    "        return \"\"\"InvoiceNumber string, CreatedTime bigint, StoreID string, PosID string, CashierID string,\n",
    "                CustomerType string, CustomerCardNo string, TotalAmount double, NumberOfItems bigint, \n",
    "                PaymentMethod string, TaxableAmount double, CGST double, SGST double, CESS double, \n",
    "                DeliveryType string,\n",
    "                DeliveryAddress struct<AddressLine string, City string, ContactNumber string, PinCode string, \n",
    "                State string>,\n",
    "                InvoiceLineItems array<struct<ItemCode string, ItemDescription string, \n",
    "                    ItemPrice double, ItemQty bigint, TotalValue double>>\n",
    "            \"\"\"\n",
    "    \n",
    "    def readInvoice(self):\n",
    "        from pyspark.sql.functions import input_file_name\n",
    "        return (spark.readStream\n",
    "                    .format(\"json\")\n",
    "                    .schema(self.getSchema())\n",
    "                    .load(f\"{self.base_data_dir}/data/aggregate/invoices\")\n",
    "                    .withColumn(\"InputFile\", input_file_name())\n",
    "                )\n",
    "    \n",
    "    def process(self):\n",
    "        print(f\"\\nStarting Bronze Stream...\", end=\"\")\n",
    "        invoiceDF = self.readInvoice() \n",
    "        sQuery = ( invoiceDF.writeStream\n",
    "                        .queryName(\"bronze-ingestion\")\n",
    "                        .option(\"checkpointLocation\", f\"{self.base_data_dir}/checkpoint/invoices_bz\")\n",
    "                        .outputMode(\"append\")\n",
    "                        .toTable(\"invoices_bz\")\n",
    "                )\n",
    "        print(\"Done\")\n",
    "        return sQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8ecc2db-9a47-427e-898f-b5d04e28d4dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Gold():\n",
    "    def __init__(self):\n",
    "        self.base_data_dir = \"/FileStore/data_spark_streaming\"\n",
    "\n",
    "    def readBronze(self):\n",
    "        return spark.readStream.table(\"invoices_bz\")\n",
    "\n",
    "    def getAggregates(self, invoices_df):\n",
    "        from pyspark.sql.functions import sum, expr\n",
    "        return (invoices_df.groupBy(\"CustomerCardNo\")\n",
    "                    .agg(sum(\"TotalAmount\").alias(\"TotalAmount\"),\n",
    "                         sum(expr(\"TotalAmount*0.02\")).alias(\"TotalPoints\"))\n",
    "        )\n",
    "\n",
    "    def upsert(self, rewards_df, batch_id):\n",
    "        rewards_df.createOrReplaceTempView(\"customer_rewards_df_temp_view\")\n",
    "        merge_statement = \"\"\"\n",
    "        MERGE INTO customer_rewards t\n",
    "        USING customer_rewards_df_temp_view s\n",
    "        ON s.CustomerCardNo = t.CustomerCardNo\n",
    "        WHEN MATCHED THEN\n",
    "        UPDATE SET t.TotalAmount = s.TotalAmount, t.TotalPoints = s.TotalPoints\n",
    "        WHEN NOT MATCHED THEN\n",
    "        INSERT (CustomerCardNo, TotalAmount, TotalPoints) VALUES (s.CustomerCardNo, s.TotalAmount, s.TotalPoints)\n",
    "        \"\"\"\n",
    "        rewards_df._jdf.sparkSession().sql(merge_statement)\n",
    "\n",
    "    def saveResults(self, results_df):\n",
    "        print(f\"\\nStarting Silver Stream...\", end='')\n",
    "        return (results_df.writeStream\n",
    "                    .queryName(\"gold-update\")\n",
    "                    .option(\"checkpointLocation\", f\"{self.base_data_dir}/checkpoint/customer_rewards\")\n",
    "                    .outputMode(\"update\")\n",
    "                    .foreachBatch(self.upsert)\n",
    "                    .start()\n",
    "                )\n",
    "\n",
    "    def process(self):\n",
    "        invoices_df = self.readBronze()\n",
    "        aggregate_df = self.getAggregates(invoices_df)\n",
    "        sQuery = self.saveResults(aggregate_df)\n",
    "        return sQuery\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "14-streaming-incremental-aggregates",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
