{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6bdfaf9-3c35-4151-bd2b-7db1c3993668",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # cleanup and setup\n",
    "\n",
    "# base_data_dir = \"/FileStore/data_spark_streaming\"\n",
    "\n",
    "# spark.sql(\"drop table if exists word_count_table\")\n",
    "\n",
    "# dbutils.fs.rm(\"/user/hive/warehouse/word_count_table\", True)\n",
    "\n",
    "# dbutils.fs.rm(f\"{base_data_dir}/checkpoint\", True)\n",
    "# dbutils.fs.rm(f\"{base_data_dir}/data/text\", True)\n",
    "\n",
    "# dbutils.fs.mkdirs(f\"{base_data_dir}/data/text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65ba7ab1-0429-49f5-b86a-95178013d6bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01-streaming-word-count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e7fd367-e25b-4727-9305-da9574bdcaa1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/data_spark_streaming/data/text/</td><td>text/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/data_spark_streaming/data/text_data_1.txt</td><td>text_data_1.txt</td><td>874</td><td>1722393374000</td></tr><tr><td>dbfs:/FileStore/data_spark_streaming/data/text_data_2.txt</td><td>text_data_2.txt</td><td>623</td><td>1722393373000</td></tr><tr><td>dbfs:/FileStore/data_spark_streaming/data/text_data_3.txt</td><td>text_data_3.txt</td><td>328</td><td>1722393373000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/data_spark_streaming/data/text/",
         "text/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/data_spark_streaming/data/text_data_1.txt",
         "text_data_1.txt",
         874,
         1722393374000
        ],
        [
         "dbfs:/FileStore/data_spark_streaming/data/text_data_2.txt",
         "text_data_2.txt",
         623,
         1722393373000
        ],
        [
         "dbfs:/FileStore/data_spark_streaming/data/text_data_3.txt",
         "text_data_3.txt",
         328,
         1722393373000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls('/FileStore/data_spark_streaming/data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8722edd0-7fd4-42b0-9ef9-f7a0f678690c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53176c9a-cfec-4747-a566-86de931b63cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class batchWCTestSuite():\n",
    "    def __init__(self):\n",
    "        self.base_data_dir = \"/FileStore/data_spark_streaming\"\n",
    "\n",
    "    def cleanTests(self):\n",
    "        print(f\"Starting Cleanup...\", end='')\n",
    "        spark.sql(\"drop table if exists word_count_table\")\n",
    "        dbutils.fs.rm(\"/user/hive/warehouse/word_count_table\", True)\n",
    "\n",
    "        dbutils.fs.rm(f\"{self.base_data_dir}/checkpoint\", True)\n",
    "        dbutils.fs.rm(f\"{self.base_data_dir}/data/text\", True)\n",
    "\n",
    "        dbutils.fs.mkdirs(f\"{self.base_data_dir}/data/text\")\n",
    "        print(\"Done\\n\")\n",
    "\n",
    "    def ingestData(self, itr):\n",
    "        print(f\"\\tStarting Ingestion...\", end='')\n",
    "        dbutils.fs.cp(f\"{self.base_data_dir}/data/text_data_{itr}.txt\", f\"{self.base_data_dir}/data/text/\")\n",
    "        print(\"Done\")\n",
    "    def assertResult(self, expected_count):\n",
    "        print(f\"\\tStarting validation...\", end='')\n",
    "        actual_count = spark.sql(\"select sum(count) from word_count_table where substr(word, 1, 1) == 's'\").collect()[0][0]\n",
    "        print(expected_count)\n",
    "        print(actual_count)\n",
    "        assert expected_count == int(actual_count), f\"Test failed! actual count is {actual_count}\"\n",
    "        print(\"Done\")\n",
    "\n",
    "    def runTests(self):\n",
    "        self.cleanTests()\n",
    "        wc = batchWC()\n",
    "\n",
    "        print(\"Testing first iteration of batch word count...\") \n",
    "        self.ingestData(1)\n",
    "        wc.wordCount()\n",
    "        self.assertResult(25)\n",
    "        print(\"First iteration of batch word count completed.\\n\")\n",
    "\n",
    "        print(\"Testing second iteration of batch word count...\") \n",
    "        self.ingestData(2)\n",
    "        wc.wordCount()\n",
    "        self.assertResult(32)\n",
    "        print(\"Second iteration of batch word count completed.\\n\") \n",
    "\n",
    "        print(\"Testing third iteration of batch word count...\") \n",
    "        self.ingestData(3)\n",
    "        wc.wordCount()\n",
    "        self.assertResult(37)\n",
    "        print(\"Third iteration of batch word count completed.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60e53bb4-ca6d-473f-95b0-b9190e2d13f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Cleanup...Done\n\nTesting first iteration of batch word count...\n\tStarting Ingestion...Done\n\tExecuting Word Count...Done\n\tStarting validation...25\n25\nDone\nFirst iteration of batch word count completed.\n\nTesting second iteration of batch word count...\n\tStarting Ingestion...Done\n\tExecuting Word Count...Done\n\tStarting validation...32\n32\nDone\nSecond iteration of batch word count completed.\n\nTesting third iteration of batch word count...\n\tStarting Ingestion...Done\n\tExecuting Word Count...Done\n\tStarting validation...37\n37\nDone\nThird iteration of batch word count completed.\n\n"
     ]
    }
   ],
   "source": [
    "bwcTS = batchWCTestSuite()\n",
    "bwcTS.runTests()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02-batch-word-count-test-suite",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
