{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c46d521b-c155-4009-bfb4-14e7211351b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./10-kafka-to-bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6b84372-bd24-4241-88d0-74f6e0ffcc27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dbutils.fs.rm(\"/FileStore/data_spark_streaming/checkpoint/invoices_kafka_bz\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2755435-19d9-46c8-82a4-3ae3f99ccd73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class kafkaToBronzeTestSuite():\n",
    "    def __init__(self):\n",
    "        self.base_data_dir = \"/FileStore/data_spark_streaming\"\n",
    "\n",
    "    def cleanTests(self):\n",
    "        print(f\"Starting Cleanup...\", end='')\n",
    "        spark.sql(\"drop table if exists invoices_bz\")\n",
    "        dbutils.fs.rm(\"/user/hive/warehouse/invoices_bz\", True)\n",
    "        dbutils.fs.rm(f\"{self.base_data_dir}/checkpoint/invoices_bz\", True)\n",
    "        print(\"Done\")\n",
    "\n",
    "    def assertResult(self, expected_count):\n",
    "        print(f\"\\tStarting validation...\", end='')\n",
    "        actual_count = spark.sql(\"select count(*) from invoices_bz\").collect()[0][0]\n",
    "        assert expected_count == actual_count, f\"Test failed! actual count is {actual_count}\"\n",
    "        print(\"Done\")\n",
    "\n",
    "    def waitForMicroBatch(self, sleep=30):\n",
    "        import time\n",
    "        print(f\"\\tWaiting for {sleep} seconds...\", end='')\n",
    "        time.sleep(sleep)\n",
    "        print(\"Done.\")    \n",
    "\n",
    "    def runTests(self):\n",
    "        self.cleanTests()\n",
    "        bzStream = Bronze()        \n",
    "\n",
    "        print(\"Testing Scenario - Start from beginning on a new checkpoint...\") \n",
    "        bzQuery = bzStream.process()\n",
    "        self.waitForMicroBatch() \n",
    "        bzQuery.stop()       \n",
    "        self.assertResult(30)\n",
    "        print(\"Validation passed.\\n\")        \n",
    "\n",
    "        print(\"Testing Scenarion - Restart from where it stopped on the same checkpoint...\")\n",
    "        bzQuery = bzStream.process()\n",
    "        self.waitForMicroBatch()\n",
    "        bzQuery.stop()\n",
    "        self.assertResult(30)\n",
    "        print(\"Validation passed.\\n\") \n",
    "\n",
    "        print(\"Testing Scenario - Start from 1697945539000 on a new checkpoint...\") \n",
    "        dbutils.fs.rm(f\"{self.base_data_dir}/checkpoint/invoices_bz\", True)\n",
    "        bzQuery = bzStream.process(1697945539000)\n",
    "        self.waitForMicroBatch()\n",
    "        bzQuery.stop()\n",
    "        self.assertResult(40)\n",
    "        print(\"Validation passed.\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7caee486-7910-48e7-ba8c-cbb538bf3177",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Cleanup...Done\nTesting Scenario - Start from beginning on a new checkpoint...\nStarting Bronze Streaming Job...\nDone\n\tWaiting for 30 seconds...Done.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-990638186221114>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m ts \u001B[38;5;241m=\u001B[39m kafkaToBronzeTestSuite()\n",
       "\u001B[0;32m----> 2\u001B[0m ts\u001B[38;5;241m.\u001B[39mrunTests()\n",
       "\n",
       "File \u001B[0;32m<command-990638186221112>, line 31\u001B[0m, in \u001B[0;36mkafkaToBronzeTestSuite.runTests\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m     29\u001B[0m bzQuery \u001B[38;5;241m=\u001B[39m bzStream\u001B[38;5;241m.\u001B[39mprocess()\n",
       "\u001B[1;32m     30\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwaitForMicroBatch() \n",
       "\u001B[0;32m---> 31\u001B[0m \u001B[43mbzQuery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m       \n",
       "\u001B[1;32m     32\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39massertResult(\u001B[38;5;241m30\u001B[39m)\n",
       "\u001B[1;32m     33\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation passed.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)        \n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/streaming/query.py:372\u001B[0m, in \u001B[0;36mStreamingQuery.stop\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    349\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstop\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    350\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    351\u001B[0m \u001B[38;5;124;03m    Stop this streaming query.\u001B[39;00m\n",
       "\u001B[1;32m    352\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    370\u001B[0m \u001B[38;5;124;03m    False\u001B[39;00m\n",
       "\u001B[1;32m    371\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 372\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:224\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    222\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    223\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 224\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    225\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    226\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o461.stop.\n",
       ": java.util.concurrent.TimeoutException: Stream Execution thread for stream bronze-ingestion [id = 8995dfd5-106d-446e-8e6b-df02fcc1ec0c, runId = e93b3b4c-47f9-4ee8-9848-099da264aefb] failed to stop within 15000 milliseconds (specified by spark.sql.streaming.stopTimeout). See the cause on what was being executed in the streaming query thread.\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution.interruptAndAwaitExecutionThreadTermination(StreamExecution.scala:710)\n",
       "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.stop(MicroBatchExecution.scala:408)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamingQueryWrapper.stop(StreamingQueryWrapper.scala:67)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: org.apache.spark.SparkException: The stream thread was last executing:\n",
       "\tat java.lang.Object.wait(Native Method)\n",
       "\tat java.lang.Thread.join(Thread.java:1265)\n",
       "\tat kafkashaded.org.apache.kafka.clients.admin.KafkaAdminClient.close(KafkaAdminClient.java:667)\n",
       "\tat kafkashaded.org.apache.kafka.clients.admin.Admin.close(Admin.java:154)\n",
       "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.stopAdmin(KafkaOffsetReaderAdmin.scala:555)\n",
       "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.close(KafkaOffsetReaderAdmin.scala:114)\n",
       "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.stop(KafkaMicroBatchStream.scala:382)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$stopSources$1(StreamExecution.scala:625)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$stopSources$1$adapted(StreamExecution.scala:623)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$10114/1783176589.apply(Unknown Source)\n",
       "\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:193)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution.stopSources(StreamExecution.scala:623)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$5(StreamExecution.scala:519)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$10113/1114223969.apply$mcV$sp(Unknown Source)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:508)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$9921/1137504172.apply$mcV$sp(Unknown Source)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat com.databricks.logging.UsageLogging$$Lambda$465/1233563882.apply(Unknown Source)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)\n",
       "\tat com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)\n",
       "\tat com.databricks.spark.util.UsageLogging$$Lambda$9922/2140471462.apply(Unknown Source)\n",
       "\tat com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)\n",
       "\tat com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)\n",
       "\tat com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)\n",
       "\tat com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution.withAttributionTags(StreamExecution.scala:84)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:378)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$3(StreamExecution.scala:282)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1$$Lambda$9920/114192657.apply$mcV$sp(Unknown Source)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$2(StreamExecution.scala:282)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1$$Lambda$9919/49407383.apply$mcV$sp(Unknown Source)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:129)\n",
       "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:281)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "Py4JJavaError",
        "evalue": "An error occurred while calling o461.stop.\n: java.util.concurrent.TimeoutException: Stream Execution thread for stream bronze-ingestion [id = 8995dfd5-106d-446e-8e6b-df02fcc1ec0c, runId = e93b3b4c-47f9-4ee8-9848-099da264aefb] failed to stop within 15000 milliseconds (specified by spark.sql.streaming.stopTimeout). See the cause on what was being executed in the streaming query thread.\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.interruptAndAwaitExecutionThreadTermination(StreamExecution.scala:710)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.stop(MicroBatchExecution.scala:408)\n\tat org.apache.spark.sql.execution.streaming.StreamingQueryWrapper.stop(StreamingQueryWrapper.scala:67)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: The stream thread was last executing:\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1265)\n\tat kafkashaded.org.apache.kafka.clients.admin.KafkaAdminClient.close(KafkaAdminClient.java:667)\n\tat kafkashaded.org.apache.kafka.clients.admin.Admin.close(Admin.java:154)\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.stopAdmin(KafkaOffsetReaderAdmin.scala:555)\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.close(KafkaOffsetReaderAdmin.scala:114)\n\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.stop(KafkaMicroBatchStream.scala:382)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$stopSources$1(StreamExecution.scala:625)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$stopSources$1$adapted(StreamExecution.scala:623)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$10114/1783176589.apply(Unknown Source)\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:193)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.stopSources(StreamExecution.scala:623)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$5(StreamExecution.scala:519)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$10113/1114223969.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:508)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$9921/1137504172.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat com.databricks.logging.UsageLogging$$Lambda$465/1233563882.apply(Unknown Source)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)\n\tat com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)\n\tat com.databricks.spark.util.UsageLogging$$Lambda$9922/2140471462.apply(Unknown Source)\n\tat com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)\n\tat com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)\n\tat com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)\n\tat com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.withAttributionTags(StreamExecution.scala:84)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:378)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$3(StreamExecution.scala:282)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1$$Lambda$9920/114192657.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$2(StreamExecution.scala:282)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1$$Lambda$9919/49407383.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:129)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:281)\n"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>Py4JJavaError</span>: An error occurred while calling o461.stop.\n: java.util.concurrent.TimeoutException: Stream Execution thread for stream bronze-ingestion [id = 8995dfd5-106d-446e-8e6b-df02fcc1ec0c, runId = e93b3b4c-47f9-4ee8-9848-099da264aefb] failed to stop within 15000 milliseconds (specified by spark.sql.streaming.stopTimeout). See the cause on what was being executed in the streaming query thread.\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.interruptAndAwaitExecutionThreadTermination(StreamExecution.scala:710)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.stop(MicroBatchExecution.scala:408)\n\tat org.apache.spark.sql.execution.streaming.StreamingQueryWrapper.stop(StreamingQueryWrapper.scala:67)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: The stream thread was last executing:\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1265)\n\tat kafkashaded.org.apache.kafka.clients.admin.KafkaAdminClient.close(KafkaAdminClient.java:667)\n\tat kafkashaded.org.apache.kafka.clients.admin.Admin.close(Admin.java:154)\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.stopAdmin(KafkaOffsetReaderAdmin.scala:555)\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.close(KafkaOffsetReaderAdmin.scala:114)\n\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.stop(KafkaMicroBatchStream.scala:382)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$stopSources$1(StreamExecution.scala:625)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$stopSources$1$adapted(StreamExecution.scala:623)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$10114/1783176589.apply(Unknown Source)\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:193)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.stopSources(StreamExecution.scala:623)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$5(StreamExecution.scala:519)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$10113/1114223969.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:508)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$9921/1137504172.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat com.databricks.logging.UsageLogging$$Lambda$465/1233563882.apply(Unknown Source)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)\n\tat com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)\n\tat com.databricks.spark.util.UsageLogging$$Lambda$9922/2140471462.apply(Unknown Source)\n\tat com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)\n\tat com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)\n\tat com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)\n\tat com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.withAttributionTags(StreamExecution.scala:84)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:378)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$3(StreamExecution.scala:282)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1$$Lambda$9920/114192657.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$2(StreamExecution.scala:282)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1$$Lambda$9919/49407383.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:129)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:281)\n"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
        "File \u001B[0;32m<command-990638186221114>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m ts \u001B[38;5;241m=\u001B[39m kafkaToBronzeTestSuite()\n\u001B[0;32m----> 2\u001B[0m ts\u001B[38;5;241m.\u001B[39mrunTests()\n",
        "File \u001B[0;32m<command-990638186221112>, line 31\u001B[0m, in \u001B[0;36mkafkaToBronzeTestSuite.runTests\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     29\u001B[0m bzQuery \u001B[38;5;241m=\u001B[39m bzStream\u001B[38;5;241m.\u001B[39mprocess()\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwaitForMicroBatch() \n\u001B[0;32m---> 31\u001B[0m \u001B[43mbzQuery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m       \n\u001B[1;32m     32\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39massertResult(\u001B[38;5;241m30\u001B[39m)\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation passed.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)        \n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/streaming/query.py:372\u001B[0m, in \u001B[0;36mStreamingQuery.stop\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    349\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstop\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    350\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    351\u001B[0m \u001B[38;5;124;03m    Stop this streaming query.\u001B[39;00m\n\u001B[1;32m    352\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    370\u001B[0m \u001B[38;5;124;03m    False\u001B[39;00m\n\u001B[1;32m    371\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 372\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:224\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    223\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 224\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    225\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    226\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
        "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o461.stop.\n: java.util.concurrent.TimeoutException: Stream Execution thread for stream bronze-ingestion [id = 8995dfd5-106d-446e-8e6b-df02fcc1ec0c, runId = e93b3b4c-47f9-4ee8-9848-099da264aefb] failed to stop within 15000 milliseconds (specified by spark.sql.streaming.stopTimeout). See the cause on what was being executed in the streaming query thread.\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.interruptAndAwaitExecutionThreadTermination(StreamExecution.scala:710)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.stop(MicroBatchExecution.scala:408)\n\tat org.apache.spark.sql.execution.streaming.StreamingQueryWrapper.stop(StreamingQueryWrapper.scala:67)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: The stream thread was last executing:\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1265)\n\tat kafkashaded.org.apache.kafka.clients.admin.KafkaAdminClient.close(KafkaAdminClient.java:667)\n\tat kafkashaded.org.apache.kafka.clients.admin.Admin.close(Admin.java:154)\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.stopAdmin(KafkaOffsetReaderAdmin.scala:555)\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.close(KafkaOffsetReaderAdmin.scala:114)\n\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.stop(KafkaMicroBatchStream.scala:382)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$stopSources$1(StreamExecution.scala:625)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$stopSources$1$adapted(StreamExecution.scala:623)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$10114/1783176589.apply(Unknown Source)\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:193)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.stopSources(StreamExecution.scala:623)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$5(StreamExecution.scala:519)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$10113/1114223969.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:508)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$9921/1137504172.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat com.databricks.logging.UsageLogging$$Lambda$465/1233563882.apply(Unknown Source)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:72)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:172)\n\tat com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)\n\tat com.databricks.spark.util.UsageLogging$$Lambda$9922/2140471462.apply(Unknown Source)\n\tat com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)\n\tat com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)\n\tat com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)\n\tat com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.withAttributionTags(StreamExecution.scala:84)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:378)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$3(StreamExecution.scala:282)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1$$Lambda$9920/114192657.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$2(StreamExecution.scala:282)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1$$Lambda$9919/49407383.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:129)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:281)\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ts = kafkaToBronzeTestSuite()\n",
    "ts.runTests()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "11-kafka-to-bronze-test-suite",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
