{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dc9d262-b4ed-4d29-9e53-adb0afc2a5c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SlidingAggregate():\n",
    "    def __init__(self):\n",
    "        self.base_data_dir = \"/FileStore/data_spark_streaming\"\n",
    "    \n",
    "    def getSchema(self):\n",
    "        from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "        return StructType([\n",
    "                    StructField(\"CreatedTime\", StringType()),\n",
    "                    StructField(\"Reading\", DoubleType())\n",
    "                ])\n",
    "        \n",
    "    def readBronze(self):\n",
    "        return spark.readStream.table(\"kafka_bz\")\n",
    "    \n",
    "    def getSensorData(self, kafka_df):\n",
    "        from pyspark.sql.functions import from_json, expr\n",
    "        return (kafka_df.select(kafka_df.key.cast(\"string\").alias(\"SensorID\"),\n",
    "                                from_json(kafka_df.value.cast(\"string\"), self.getSchema()).alias(\"value\"))\n",
    "                        .select(\"SensorID\", \"value.*\")\n",
    "                        .withColumn(\"CreatedTime\", expr(\"to_timestamp(CreatedTime, 'yyyy-MM-dd HH:mm:ss')\"))\n",
    "                )\n",
    "        \n",
    "    def getAggregate(self, sensor_df):\n",
    "        from pyspark.sql.functions import window, max\n",
    "        return (sensor_df.withWatermark(\"CreatedTime\", \"30 minutes\")\n",
    "                        .groupBy(sensor_df.SensorID,\n",
    "                                 window(sensor_df.CreatedTime, \"15 minutes\", \"5 minute\"))\n",
    "                        .agg(max(\"Reading\").alias(\"MaxReading\"))\n",
    "                        .select(\"SensorID\", \"window.start\", \"window.end\", \"MaxReading\")\n",
    "                )\n",
    "        \n",
    "    def saveResults(self, results_df):\n",
    "        print(f\"\\nStarting Silver Stream...\", end='')\n",
    "        return (results_df.writeStream\n",
    "                    .queryName(\"sensor-query\")\n",
    "                    .option(\"checkpointLocation\", f\"{self.base_data_dir}/checkpoint/sensor_summary\")\n",
    "                    .outputMode(\"complete\")\n",
    "                    .toTable(\"sensor_summary\")\n",
    "                )\n",
    "        print(\"Done\")\n",
    "\n",
    "    def process(self):\n",
    "        kafka_df = self.readBronze()\n",
    "        sensor_df = self.getSensorData(kafka_df)\n",
    "        results_df = self.getAggregate(sensor_df)        \n",
    "        sQuery = self.saveResults(results_df)\n",
    "        return sQuery\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "20-sliding-window",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
