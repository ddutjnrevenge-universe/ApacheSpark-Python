{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6bdfaf9-3c35-4151-bd2b-7db1c3993668",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # cleanup and setup\n",
    "\n",
    "# base_data_dir = \"/FileStore/data_spark_streaming\"\n",
    "\n",
    "# spark.sql(\"drop table if exists word_count_table\")\n",
    "\n",
    "# dbutils.fs.rm(\"/user/hive/warehouse/word_count_table\", True)\n",
    "\n",
    "# dbutils.fs.rm(f\"{base_data_dir}/checkpoint\", True)\n",
    "# dbutils.fs.rm(f\"{base_data_dir}/data/text\", True)\n",
    "\n",
    "# dbutils.fs.mkdirs(f\"{base_data_dir}/data/text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, trim, lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65ba7ab1-0429-49f5-b86a-95178013d6bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Word Count Test Suite\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e7fd367-e25b-4727-9305-da9574bdcaa1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the base data directory\n",
    "base_data_dir = \"./data\"  # assuming you have your data in ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53176c9a-cfec-4747-a566-86de931b63cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the batchWC class\n",
    "class batchWC():\n",
    "    def __init__(self):\n",
    "        self.base_data_dir = base_data_dir\n",
    "\n",
    "    def getRawData(self):\n",
    "        lines = (spark.read\n",
    "                 .format(\"text\")\n",
    "                 .option(\"lineSep\", \".\")\n",
    "                 .load(f\"{self.base_data_dir}/data/text\")\n",
    "                 )\n",
    "        return lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "\n",
    "    def getQualityData(self, rawDF):\n",
    "        return (rawDF.select(lower(trim(rawDF.word)).alias(\"word\"))\n",
    "                .where(\"word is not null\")\n",
    "                .where(\"word rlike '[a-z]'\"))\n",
    "\n",
    "    def getWordCount(self, qualityDF):\n",
    "        return qualityDF.groupBy(\"word\").count()\n",
    "\n",
    "    def overwriteWordCount(self, wordCountDF):\n",
    "        (wordCountDF.write\n",
    "         .format(\"parquet\")  # Changed to parquet for local filesystem\n",
    "         .mode(\"overwrite\")\n",
    "         .saveAsTable(\"word_count_table\"))\n",
    "\n",
    "    def wordCount(self):\n",
    "        print(f\"\\tExecuting Word Count...\", end=\"\")\n",
    "        rawDF = self.getRawData()\n",
    "        qualityDF = self.getQualityData(rawDF)\n",
    "        resultDF = self.getWordCount(qualityDF)\n",
    "        self.overwriteWordCount(resultDF)\n",
    "        print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch word count test suite class\n",
    "class batchWCTestSuite():\n",
    "    def __init__(self):\n",
    "        self.base_data_dir = base_data_dir\n",
    "\n",
    "    def cleanTests(self):\n",
    "        # Drop table if exists\n",
    "        spark.sql(\"drop table if exists word_count_table\")\n",
    "\n",
    "        # Remove files and directories\n",
    "        if os.path.exists(f\"{self.base_data_dir}/checkpoint\"):\n",
    "            shutil.rmtree(f\"{self.base_data_dir}/checkpoint\")\n",
    "        if os.path.exists(f\"{self.base_data_dir}/data/text\"):\n",
    "            shutil.rmtree(f\"{self.base_data_dir}/data/text\")\n",
    "\n",
    "        os.makedirs(f\"{self.base_data_dir}/data/text\")\n",
    "        print(\"Done\\n\")\n",
    "\n",
    "    def ingestData(self, itr):\n",
    "        print(f\"\\tStarting Ingestion...\", end='')\n",
    "        shutil.copy(f\"{self.base_data_dir}/text_data_{itr}.txt\", f\"{self.base_data_dir}/data/text/text_data_{itr}.txt\")\n",
    "        print(\"Done\")\n",
    "\n",
    "    def assertResult(self, expected_count):\n",
    "        actual_count = spark.sql(\"select sum(count) from word_count_table where substr(word, 1, 1) = 's'\").collect()[0][0]\n",
    "        assert expected_count == actual_count, f\"Test failed! actual count is {actual_count}\"\n",
    "\n",
    "    def runTests(self):\n",
    "        self.cleanTests()\n",
    "        wc = batchWC()  # Use the batchWC class defined above\n",
    "\n",
    "        print(\"Testing first iteration of batch word count...\")\n",
    "        self.ingestData(1)\n",
    "        wc.wordCount()\n",
    "        self.assertResult(25)\n",
    "        print(\"First iteration of batch word count completed.\\n\")\n",
    "\n",
    "        print(\"Testing second iteration of batch word count...\")\n",
    "        self.ingestData(2)\n",
    "        wc.wordCount()\n",
    "        self.assertResult(32)\n",
    "        print(\"Second iteration of batch word count completed.\\n\")\n",
    "\n",
    "        print(\"Testing third iteration of batch word count...\")\n",
    "        self.ingestData(3)\n",
    "        wc.wordCount()\n",
    "        self.assertResult(37)\n",
    "        print(\"Third iteration of batch word count completed.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60e53bb4-ca6d-473f-95b0-b9190e2d13f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "\n",
      "Testing first iteration of batch word count...\n",
      "\tStarting Ingestion...Done\n",
      "\tExecuting Word Count..."
     ]
    },
    {
     "ename": "SparkRuntimeException",
     "evalue": "[LOCATION_ALREADY_EXISTS] Cannot name the managed table as `spark_catalog`.`default`.`word_count_table`, as its associated location 'file:/F:/Data_Engineering/Apache_Spark/Spark_LakeHouse/spark-warehouse/word_count_table' already exists. Please pick a different table name, or remove the existing location first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSparkRuntimeException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m bwcTS \u001b[38;5;241m=\u001b[39m batchWCTestSuite()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mbwcTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunTests\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 34\u001b[0m, in \u001b[0;36mbatchWCTestSuite.runTests\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting first iteration of batch word count...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mingestData(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwordCount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massertResult(\u001b[38;5;241m25\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst iteration of batch word count completed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 33\u001b[0m, in \u001b[0;36mbatchWC.wordCount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     31\u001b[0m qualityDF \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetQualityData(rawDF)\n\u001b[0;32m     32\u001b[0m resultDF \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetWordCount(qualityDF)\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverwriteWordCount\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresultDF\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m, in \u001b[0;36mbatchWC.overwriteWordCount\u001b[1;34m(self, wordCountDF)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moverwriteWordCount\u001b[39m(\u001b[38;5;28mself\u001b[39m, wordCountDF):\n\u001b[1;32m---> 23\u001b[0m     (\u001b[43mwordCountDF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Changed to parquet for local filesystem\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mword_count_table\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mD:\\spark\\python\\pyspark\\sql\\readwriter.py:1586\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[1;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m-> 1586\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mD:\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mSparkRuntimeException\u001b[0m: [LOCATION_ALREADY_EXISTS] Cannot name the managed table as `spark_catalog`.`default`.`word_count_table`, as its associated location 'file:/F:/Data_Engineering/Apache_Spark/Spark_LakeHouse/spark-warehouse/word_count_table' already exists. Please pick a different table name, or remove the existing location first."
     ]
    }
   ],
   "source": [
    "bwcTS = batchWCTestSuite()\n",
    "bwcTS.runTests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6c5a4d5-07ff-4c71-b0c4-c0c8be4f6bf8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02-word-count-test-suite",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
