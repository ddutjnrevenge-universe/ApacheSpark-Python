{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./14-streaming-incremental-aggregates.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "                    .appName(\"Streaming Incremental Aggregates\")\\\n",
    "                    .getOrCreate()\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"MedallionApproachTest\") \\\n",
    "#     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.1.0\") \\\n",
    "#     .getOrCreate()\n",
    "# spark = SparkSession.builder \\\n",
    "#         .appName(\"Streaming Incremental Aggregates\") \\\n",
    "#         .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "#         .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.1.0\") \\\n",
    "#         .master(\"local[*]\") \\\n",
    "#         .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregationTestSuite():\n",
    "    def __init__(self):\n",
    "        self.base_data_dir = \"data/invoices\"\n",
    "        self.spark_warehouse_dir = \"spark-warehouse\"\n",
    "\n",
    "    def cleanTests(self):\n",
    "        print(f\"Starting Cleanup...\", end='')\n",
    "        spark.sql(\"drop table if exists invoices_bz\")\n",
    "        spark.sql(\"drop table if exists customer_rewards\")\n",
    "\n",
    "        def remove_dir(path):\n",
    "            if os.path.exists(path):\n",
    "                shutil.rmtree(path)\n",
    "\n",
    "        remove_dir(f\"{self.spark_warehouse_dir}/invoices_bz\")\n",
    "        remove_dir(f\"{self.spark_warehouse_dir}/customer_rewards\")\n",
    "        # spark.sql(f\"CREATE TABLE customer_rewards(CustomerCardNo STRING, TotalAmount DOUBLE, TotalPoints DOUBLE)\")\n",
    "\n",
    "        remove_dir(f\"{self.base_data_dir}/checkpoint/invoices_bz\")\n",
    "        remove_dir(f\"{self.base_data_dir}/checkpoint/customer_rewards\")\n",
    "\n",
    "        remove_dir(f\"{self.base_data_dir}/data/aggregate/invoices\")\n",
    "        print(\"Done removing directories.\")\n",
    "        os.makedirs(f\"{self.base_data_dir}/data/aggregate/invoices\")\n",
    "        print(\"Done\")\n",
    "\n",
    "    def ingestData(self, itr):\n",
    "        print(f\"\\tStarting Ingestion...\", end='')\n",
    "        shutil.copy(f\"{self.base_data_dir}/invoices-{itr}.json\", f\"{self.base_data_dir}/data/aggregate/invoices/\")\n",
    "        print(\"Done\")\n",
    "\n",
    "    def assertBronze(self, expected_count):\n",
    "        print(f\"\\tStarting Bronze validation...\", end='')\n",
    "        actual_count = spark.sql(\"select count(*) from invoices_bz\").collect()[0][0]\n",
    "        assert expected_count == actual_count, f\"Test failed! actual count is {actual_count}\"\n",
    "        print(\"Done\")\n",
    "\n",
    "    def assertGold(self, expected_value):\n",
    "        print(f\"\\tStarting Gold validation...\", end='')\n",
    "        actual_value = spark.sql(\"select TotalAmount from customer_rewards\").filter(\"CustomerCardNo = '2262471989'\").collect()[0][0]\n",
    "        assert expected_value == actual_value, f\"Test failed! actual value is {actual_value}\"\n",
    "        print(\"Done\")\n",
    "\n",
    "    def waitForMicroBatch(self, sleep=120):\n",
    "        import time\n",
    "        print(f\"\\tWaiting for {sleep} seconds...\", end='')\n",
    "        time.sleep(sleep)\n",
    "        print(\"Done.\")    \n",
    "\n",
    "    def runTests(self):\n",
    "        self.cleanTests()\n",
    "        # Ensure the customer_rewards table is created\n",
    "        # create in memory table in spark-warehouse\n",
    "\n",
    "        spark.conf.set(\"spark.sql.streaming.stateStore.rpoviderClass\",\n",
    "                       \"org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider\")\n",
    "        bzStream = Bronze()\n",
    "        bzQuery = bzStream.process()\n",
    "        gdStream = Gold()\n",
    "        gdQuery = gdStream.process()       \n",
    "\n",
    "        print(\"\\nTesting first iteration of invoice stream...\") \n",
    "        self.ingestData(1)\n",
    "        self.waitForMicroBatch()        \n",
    "        self.assertBronze(501)\n",
    "        self.assertGold(36859)\n",
    "        print(\"Validation passed.\\n\")\n",
    "\n",
    "        print(\"\\nTesting second iteration of invoice stream...\") \n",
    "        self.ingestData(2)\n",
    "        self.waitForMicroBatch()        \n",
    "        self.assertBronze(501+500)\n",
    "        self.assertGold(36859+20740)\n",
    "        print(\"Validation passed.\\n\")\n",
    "\n",
    "        print(\"\\nTesting second iteration of invoice stream...\") \n",
    "        self.ingestData(3)\n",
    "        self.waitForMicroBatch()        \n",
    "        self.assertBronze(501+500+590)\n",
    "        self.assertGold(36859+20740+31959)\n",
    "        print(\"Validation passed.\\n\")\n",
    "\n",
    "        bzQuery.stop()\n",
    "        gdQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Cleanup...Done removing directories.\n",
      "Done\n",
      "\n",
      "Starting Bronze Stream...Done\n",
      "\n",
      "Starting Silver Stream...\n",
      "Testing first iteration of invoice stream...\n",
      "\tStarting Ingestion...Done\n",
      "\tWaiting for 120 seconds...Done.\n",
      "\tStarting Bronze validation...Done\n",
      "\tStarting Gold validation..."
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `customer_rewards` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 24;\n'Project ['TotalAmount]\n+- 'UnresolvedRelation [customer_rewards], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m aTS \u001b[38;5;241m=\u001b[39m AggregationTestSuite()\n\u001b[1;32m----> 2\u001b[0m \u001b[43maTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunTests\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\t\n",
      "Cell \u001b[1;32mIn[5], line 66\u001b[0m, in \u001b[0;36mAggregationTestSuite.runTests\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaitForMicroBatch()        \n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massertBronze(\u001b[38;5;241m501\u001b[39m)\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massertGold\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m36859\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation passed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTesting second iteration of invoice stream...\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n",
      "Cell \u001b[1;32mIn[5], line 40\u001b[0m, in \u001b[0;36mAggregationTestSuite.assertGold\u001b[1;34m(self, expected_value)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21massertGold\u001b[39m(\u001b[38;5;28mself\u001b[39m, expected_value):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mStarting Gold validation...\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m     actual_value \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mselect TotalAmount from customer_rewards\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomerCardNo = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2262471989\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m expected_value \u001b[38;5;241m==\u001b[39m actual_value, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest failed! actual value is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\spark\\python\\pyspark\\sql\\session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[1;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[0;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[0;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[0;32m   1630\u001b[0m         )\n\u001b[1;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mD:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mD:\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `customer_rewards` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 24;\n'Project ['TotalAmount]\n+- 'UnresolvedRelation [customer_rewards], [], false\n"
     ]
    }
   ],
   "source": [
    "aTS = AggregationTestSuite()\n",
    "aTS.runTests()\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
