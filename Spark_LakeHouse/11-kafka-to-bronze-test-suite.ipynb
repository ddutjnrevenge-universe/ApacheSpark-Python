{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./10-kafka-to-bronze.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/06 10:54:30 WARN Utils: Your hostname, Min resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/08/06 10:54:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ddutjnrevenge/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ddutjnrevenge/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ddutjnrevenge/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a4d010a6-e4b1-4789-a7b9-d5f13bd3b3b5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.3.2 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.9.1 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.6 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 354ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.3.2 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.4.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.6 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.9.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a4d010a6-e4b1-4789-a7b9-d5f13bd3b3b5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/6ms)\n",
      "24/08/06 10:54:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "                    .appName(\"KafkaBronze\")\\\n",
    "                    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kafkaToBronzeTestSuite():\n",
    "    def __init__(self):\n",
    "        self.base_data_dir = \"data\"\n",
    "        self.spark_warehouse_dir = \"spark-warehouse\"\n",
    "\n",
    "    def getTomorrowTimestamp(self):\n",
    "        tomorrow = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) + timedelta(days=2)\n",
    "        return int(time.mktime(tomorrow.timetuple()) * 1000)\n",
    "\n",
    "    def cleanTests(self):\n",
    "        print(f\"Starting Cleanup...\", end='')\n",
    "        spark.sql(\"drop table if exists invoices_bz\")\n",
    "\n",
    "        def remove_dir(path):\n",
    "            if os.path.exists(path):\n",
    "                shutil.rmtree(path)\n",
    "        # remove dir in spark-warehouse\n",
    "        print(\"Start removing tables in spark-warehouse\")\n",
    "        remove_dir(f\"{self.spark_warehouse_dir}/invoices_bz\")\n",
    "        print(\"Start removing data in checkpoint\")\n",
    "        remove_dir(f\"{self.base_data_dir}/checkpoint/invoices_bz\")\n",
    "        print(\"Done\")\n",
    "\n",
    "    def assertResult(self, expected_count):\n",
    "        print(f\"\\tStarting validation...\", end='')\n",
    "        actual_count = spark.sql(\"select count(*) from invoices_bz\").collect()[0][0]\n",
    "        assert expected_count == actual_count, f\"Test failed! actual count is {actual_count}\"\n",
    "        print(\"Done\")\n",
    "\n",
    "    def waitForMicroBatch(self, sleep=30):\n",
    "        print(f\"\\tWaiting for {sleep} seconds...\", end='')\n",
    "        time.sleep(sleep)\n",
    "        print(\"Done.\")    \n",
    "\n",
    "    def runTests(self):        \n",
    "        self.cleanTests()\n",
    "        bzStream = Bronze()        \n",
    "\n",
    "        print(\"Testing Scenario - Start from beginning on a new checkpoint...\") \n",
    "        bzQuery = bzStream.process()\n",
    "        self.waitForMicroBatch() \n",
    "        bzQuery.stop()       \n",
    "        self.assertResult(1590)\n",
    "        print(\"Validation passed.\\n\")        \n",
    "\n",
    "        print(\"Testing Scenario - Restart from where it stopped on the same checkpoint...\")\n",
    "        bzQuery = bzStream.process()\n",
    "        self.waitForMicroBatch()\n",
    "        bzQuery.stop()\n",
    "        self.assertResult(1590)\n",
    "        print(\"Validation passed.\\n\") \n",
    "\n",
    "        # Get timestamp for tomorrow\n",
    "        tomorrow_timestamp = self.getTomorrowTimestamp()\n",
    "        print(f\"Testing Scenario - Start from {tomorrow_timestamp} on a new checkpoint...\") \n",
    "        \n",
    "        def remove_dir(path):\n",
    "            if os.path.exists(path):\n",
    "                shutil.rmtree(path)\n",
    "\n",
    "        print(\"Start removing data in checkpoint\")\n",
    "        remove_dir(f\"{self.base_data_dir}/checkpoint/invoices_bz\")\n",
    "        print(\"Done\")\n",
    "\n",
    "        bzQuery = bzStream.process(tomorrow_timestamp)\n",
    "        self.waitForMicroBatch()\n",
    "        bzQuery.stop()\n",
    "        self.assertResult(1590)\n",
    "        print(\"Validation passed.\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Cleanup...Start removing tables in spark-warehouse\n",
      "Start removing data in checkpoint\n",
      "Done\n",
      "Testing Scenario - Start from beginning on a new checkpoint...\n",
      "Starting Bronze Streaming Job...Done\n",
      "\tWaiting for 30 seconds..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/06 10:59:27 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/08/06 10:59:27 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "\tStarting validation...Done\n",
      "Validation passed.\n",
      "\n",
      "Testing Scenario - Restart from where it stopped on the same checkpoint...\n",
      "Starting Bronze Streaming Job...Done\n",
      "\tWaiting for 30 seconds..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/06 10:59:58 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/08/06 10:59:58 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "\tStarting validation...Done\n",
      "Validation passed.\n",
      "\n",
      "Testing Scenario - Start from 1723050000000 on a new checkpoint...\n",
      "Start removing data in checkpoint\n",
      "Done\n",
      "Starting Bronze Streaming Job...Done\n",
      "\tWaiting for 30 seconds..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/06 11:00:28 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/08/06 11:00:28 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "\tStarting validation...Done\n",
      "Validation passed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ts = kafkaToBronzeTestSuite()\n",
    "ts.runTests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
