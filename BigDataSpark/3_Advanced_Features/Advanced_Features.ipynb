{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hive Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Spark features\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove existing directories if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import rmtree\n",
    "import os\n",
    "\n",
    "# Define the locations\n",
    "emp_location = 'spark-warehouse/hive_emp'\n",
    "dept_location = 'spark-warehouse/hive_dept'\n",
    "\n",
    "# Remove the existing directories if they exist\n",
    "if os.path.exists(emp_location):\n",
    "    rmtree(emp_location)\n",
    "if os.path.exists(dept_location):\n",
    "    rmtree(dept_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "emp = [(1, \"Smith\", \"fi\", 1000),\n",
    "        (2, \"Rose\", \"ma\", 2000),\n",
    "        (3, \"Williams\", \"ma\", 1000),\n",
    "        (4, \"Jones\", \"sa\", 2000),\n",
    "        (5, \"Brown\", \"sa\", 1000),\n",
    "        (6, \"Katie\", \"fi\", 2000),\n",
    "        (7, \"Linda\", \"it\", 2000),\n",
    "        (8, \"Michael\", \"it\", 1000),\n",
    "        (9, \"Johnson\", \"ma\", 1000),\n",
    "        (10, \"Tom\", \"fi\", 2000)]\n",
    "\n",
    "dept = [(\"Finance\", \"fi\"),\n",
    "        (\"Marketing\", \"ma\"),\n",
    "        (\"Sales\", \"sa\"),\n",
    "        (\"Computer Science\", \"cs\"),\n",
    "        (\"Info Tech\", \"it\")]\n",
    "df = spark.createDataFrame(emp, [\"emp_id\", \"name\", \"dept_id\", \"salary\"])\n",
    "deptdf = spark.createDataFrame(dept, [\"name\", \"dept_id\"])\n",
    "\n",
    "# Create Temp Tables for SQL\n",
    "df.createOrReplaceTempView(\"empdf\")\n",
    "deptdf.createOrReplaceTempView(\"deptdf\")\n",
    "\n",
    "# Drop the existing tables if they exist\n",
    "spark.sql(\"DROP TABLE IF EXISTS hive_emp\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS hive_dept\")\n",
    "\n",
    "# Save as hive table\n",
    "df.write.saveAsTable(\"hive_emp\", mode=\"overwrite\")\n",
    "deptdf.write.saveAsTable(\"hive_dept\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast Join\n",
    "Size of broadcast table is 10MB.\n",
    "Can change threshold up to 8GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default size of broadcast table is 10.0 MB.\n"
     ]
    }
   ],
   "source": [
    "# Check size of transmission table\n",
    "threshold_str = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "threshold_value = int(threshold_str.rstrip('b'))\n",
    "size = threshold_value / (1024 * 1024)\n",
    "print(f\"Default size of broadcast table is {size} MB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set size of streaming table as 50mb\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 50 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = df.join(broadcast(deptdf), df[\"dept_id\"] == deptdf[\"dept_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching\n",
    "Use cache/persistence function to keep the dataframe in memoery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used: True\n",
      "Disk used: True\n",
      "Memory used after re-cache: True\n",
      "Disk used after re-cache: True\n"
     ]
    }
   ],
   "source": [
    "# Cache the DataFrame\n",
    "df.cache()\n",
    "\n",
    "# Trigger an action to ensure caching is materialized\n",
    "df.count()\n",
    "\n",
    "# Print the storage levels\n",
    "print(\"Memory used: {0}\".format(df.storageLevel.useMemory))\n",
    "print(\"Disk used: {0}\".format(df.storageLevel.useDisk))\n",
    "\n",
    "# You can cache again and re-check the storage level if necessary, but it's not needed\n",
    "# df.cache()\n",
    "# df.count()\n",
    "# Print the storage levels again if you want to check after caching again\n",
    "print(\"Memory used after re-cache: {0}\".format(df.storageLevel.useMemory))\n",
    "print(\"Disk used after re-cache: {0}\".format(df.storageLevel.useDisk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When use the `cache()` function, it will use storage tier as Memory_Only (~2.0.2) and Memory_and_DISK (2.1.x afterwards) \n",
    "But we can use `persist()` to specify levels of storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used for deptdf: True\n",
      "Disk used for deptdf: False\n"
     ]
    }
   ],
   "source": [
    "deptdf.persist(StorageLevel.MEMORY_ONLY)\n",
    "deptdf.count()\n",
    "print(\"Memory used for deptdf: {0}\".format(deptdf.storageLevel.useMemory))\n",
    "print(\"Disk used for deptdf: {0}\".format(deptdf.storageLevel.useDisk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dont persist\n",
    "Clear cache of data when no longer needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[emp_id: bigint, name: string, dept_id: string, salary: bigint]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, dept_id: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deptdf.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute '_wrapped'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sqlContext \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute '_wrapped'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cache\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
