{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hive Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Spark features\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove existing directories if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import rmtree\n",
    "import os\n",
    "\n",
    "# Define the locations\n",
    "emp_location = 'spark-warehouse/hive_emp'\n",
    "dept_location = 'spark-warehouse/hive_dept'\n",
    "\n",
    "# Remove the existing directories if they exist\n",
    "if os.path.exists(emp_location):\n",
    "    rmtree(emp_location)\n",
    "if os.path.exists(dept_location):\n",
    "    rmtree(dept_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+\n",
      "|emp_id|    name|dept_id|salary|\n",
      "+------+--------+-------+------+\n",
      "|     1|   Smith|     fi|  1000|\n",
      "|     2|    Rose|     ma|  2000|\n",
      "|     3|Williams|     ma|  7000|\n",
      "|     4|   Jones|     sa|  2000|\n",
      "|     5|   Brown|     sa|  1000|\n",
      "|     6|   Katie|     fi|  3500|\n",
      "|     7|   Linda|     it|  2000|\n",
      "|     8| Michael|     it|  2000|\n",
      "|     9| Johnson|     ma|  1000|\n",
      "|    10|     Tom|     fi|  4300|\n",
      "|    11|    John|     fi| 10000|\n",
      "|    12|     Doe|     ma|  5000|\n",
      "|    13|     Ben|     sa|  3000|\n",
      "|    14|  Justin|     it|  5000|\n",
      "|    15|    Lily|     cs|   100|\n",
      "|  NULL|    NULL|   NULL|  1000|\n",
      "|    17|    Jane|   NULL|  9000|\n",
      "|    18|    Lisa|   NULL|  2500|\n",
      "|    19|    Lucy|   NULL|  5000|\n",
      "|    20|    Kate|   NULL|  3000|\n",
      "+------+--------+-------+------+\n",
      "\n",
      "+----------------+-------+\n",
      "|            name|dept_id|\n",
      "+----------------+-------+\n",
      "|         Finance|     fi|\n",
      "|       Marketing|     ma|\n",
      "|           Sales|     sa|\n",
      "|Computer Science|     cs|\n",
      "|       Info Tech|     it|\n",
      "+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Create DataFrames\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema for the emp DataFrame\n",
    "emp_schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"dept_id\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Define the schema for the dept DataFrame\n",
    "dept_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"dept_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create a DataFrame for employees\n",
    "emp = [(1, \"Smith\", \"fi\", 1000),\n",
    "       (2, \"Rose\", \"ma\", 2000),\n",
    "       (3, \"Williams\", \"ma\", 7000),\n",
    "       (4, \"Jones\", \"sa\", 2000),\n",
    "       (5, \"Brown\", \"sa\", 1000),\n",
    "       (6, \"Katie\", \"fi\", 3500),\n",
    "       (7, \"Linda\", \"it\", 2000),\n",
    "       (8, \"Michael\", \"it\", 2000),\n",
    "       (9, \"Johnson\", \"ma\", 1000),\n",
    "       (10, \"Tom\", \"fi\", 4300),\n",
    "       (11, \"John\", \"fi\", 10000),\n",
    "       (12, \"Doe\", \"ma\", 5000),\n",
    "       (13, \"Ben\", \"sa\", 3000),\n",
    "       (14, \"Justin\", \"it\", 5000),\n",
    "       (15, \"Lily\", \"cs\", 100),\n",
    "       (None, None, None, 1000),\n",
    "       (17, \"Jane\", None, 9000),\n",
    "       (18, \"Lisa\", None, 2500),\n",
    "       (19, \"Lucy\", None, 5000),\n",
    "       (20, \"Kate\", None, 3000)]\n",
    "\n",
    "# Create a DataFrame for departments\n",
    "dept = [(\"Finance\", \"fi\"),\n",
    "        (\"Marketing\", \"ma\"),\n",
    "        (\"Sales\", \"sa\"),\n",
    "        (\"Computer Science\", \"cs\"),\n",
    "        (\"Info Tech\", \"it\")]\n",
    "\n",
    "# Create the DataFrames\n",
    "df = spark.createDataFrame(emp, emp_schema)\n",
    "deptdf = spark.createDataFrame(dept, dept_schema)\n",
    "\n",
    "# Create Temp Tables for SQL\n",
    "df.createOrReplaceTempView(\"empdf\")\n",
    "deptdf.createOrReplaceTempView(\"deptdf\")\n",
    "\n",
    "# Drop the existing tables if they exist\n",
    "spark.sql(\"DROP TABLE IF EXISTS hive_emp\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS hive_dept\")\n",
    "\n",
    "# Save as Hive table\n",
    "df.write.saveAsTable(\"hive_emp\", mode=\"overwrite\")\n",
    "deptdf.write.saveAsTable(\"hive_dept\", mode=\"overwrite\")\n",
    "\n",
    "# Show the DataFrames to verify\n",
    "df.show()\n",
    "deptdf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast Join\n",
    "Size of broadcast table is 10MB.\n",
    "Can change threshold up to 8GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default size of broadcast table is 50.0 MB.\n"
     ]
    }
   ],
   "source": [
    "# Check size of transmission table\n",
    "threshold_str = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "threshold_value = int(threshold_str.rstrip('b'))\n",
    "size = threshold_value / (1024 * 1024)\n",
    "print(f\"Default size of broadcast table is {size} MB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set size of streaming table as 50mb\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 50 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = df.join(broadcast(deptdf), df[\"dept_id\"] == deptdf[\"dept_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching\n",
    "Use cache/persistence function to keep the dataframe in memoery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used: True\n",
      "Disk used: True\n",
      "Memory used after re-cache: True\n",
      "Disk used after re-cache: True\n"
     ]
    }
   ],
   "source": [
    "# Cache the DataFrame\n",
    "df.cache()\n",
    "\n",
    "# Trigger an action to ensure caching is materialized\n",
    "df.count()\n",
    "\n",
    "# Print the storage levels\n",
    "print(\"Memory used: {0}\".format(df.storageLevel.useMemory))\n",
    "print(\"Disk used: {0}\".format(df.storageLevel.useDisk))\n",
    "\n",
    "# You can cache again and re-check the storage level if necessary, but it's not needed\n",
    "# df.cache()\n",
    "# df.count()\n",
    "# Print the storage levels again if you want to check after caching again\n",
    "print(\"Memory used after re-cache: {0}\".format(df.storageLevel.useMemory))\n",
    "print(\"Disk used after re-cache: {0}\".format(df.storageLevel.useDisk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When use the `cache()` function, it will use storage tier as Memory_Only (~2.0.2) and Memory_and_DISK (2.1.x afterwards) \n",
    "But we can use `persist()` to specify levels of storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used for deptdf: True\n",
      "Disk used for deptdf: False\n"
     ]
    }
   ],
   "source": [
    "deptdf.persist(StorageLevel.MEMORY_ONLY)\n",
    "deptdf.count()\n",
    "print(\"Memory used for deptdf: {0}\".format(deptdf.storageLevel.useMemory))\n",
    "print(\"Disk used for deptdf: {0}\".format(deptdf.storageLevel.useDisk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dont persist\n",
    "Clear cache of data when no longer needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[emp_id: int, name: string, dept_id: string, salary: int]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, dept_id: string]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deptdf.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cache\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+------------+\n",
      "|emp_id|    name|dept_id|salary|salary_level|\n",
      "+------+--------+-------+------+------------+\n",
      "|     1|   Smith|     fi|  1000|     invalid|\n",
      "|     2|    Rose|     ma|  2000|         low|\n",
      "|     3|Williams|     ma|  7000|        high|\n",
      "|     4|   Jones|     sa|  2000|         low|\n",
      "|     5|   Brown|     sa|  1000|     invalid|\n",
      "|     6|   Katie|     fi|  3500|      medium|\n",
      "|     7|   Linda|     it|  2000|         low|\n",
      "|     8| Michael|     it|  2000|         low|\n",
      "|     9| Johnson|     ma|  1000|     invalid|\n",
      "|    10|     Tom|     fi|  4300|      medium|\n",
      "|    11|    John|     fi| 10000|        high|\n",
      "|    12|     Doe|     ma|  5000|      medium|\n",
      "|    13|     Ben|     sa|  3000|         low|\n",
      "|    14|  Justin|     it|  5000|      medium|\n",
      "|    15|    Lily|     cs|   100|     invalid|\n",
      "|  NULL|    NULL|   NULL|  1000|     invalid|\n",
      "|    17|    Jane|   NULL|  9000|        high|\n",
      "|    18|    Lisa|   NULL|  2500|         low|\n",
      "|    19|    Lucy|   NULL|  5000|      medium|\n",
      "|    20|    Kate|   NULL|  3000|         low|\n",
      "+------+--------+-------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "script = \"\"\"case when salary > 5000 then 'high'\n",
    "                when salary > 3000 then 'medium'\n",
    "                when salary > 1000 then 'low'\n",
    "                else 'invalid'\n",
    "        end as salary_level\"\"\"\n",
    "\n",
    "newdf =   df.withColumn(\"salary_level\", expr(script))\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the selectExpr func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+------------+\n",
      "|emp_id|    name|dept_id|salary|salary_level|\n",
      "+------+--------+-------+------+------------+\n",
      "|     1|   Smith|     fi|  1000|     invalid|\n",
      "|     2|    Rose|     ma|  2000|         low|\n",
      "|     3|Williams|     ma|  7000|        high|\n",
      "|     4|   Jones|     sa|  2000|         low|\n",
      "|     5|   Brown|     sa|  1000|     invalid|\n",
      "|     6|   Katie|     fi|  3500|      medium|\n",
      "|     7|   Linda|     it|  2000|         low|\n",
      "|     8| Michael|     it|  2000|         low|\n",
      "|     9| Johnson|     ma|  1000|     invalid|\n",
      "|    10|     Tom|     fi|  4300|      medium|\n",
      "|    11|    John|     fi| 10000|        high|\n",
      "|    12|     Doe|     ma|  5000|      medium|\n",
      "|    13|     Ben|     sa|  3000|         low|\n",
      "|    14|  Justin|     it|  5000|      medium|\n",
      "|    15|    Lily|     cs|   100|     invalid|\n",
      "|  NULL|    NULL|   NULL|  1000|     invalid|\n",
      "|    17|    Jane|   NULL|  9000|        high|\n",
      "|    18|    Lisa|   NULL|  2500|         low|\n",
      "|    19|    Lucy|   NULL|  5000|      medium|\n",
      "|    20|    Kate|   NULL|  3000|         low|\n",
      "+------+--------+-------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.selectExpr(\"*\", script)\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Defined Functions (UDFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python function to find salary_level for a given salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salary_level(sal):\n",
    "    level = None\n",
    "    if sal > 5000:\n",
    "        level = 'high'\n",
    "    elif sal > 3000:\n",
    "        level = 'medium'\n",
    "    elif sal > 1000:\n",
    "        level = 'low'\n",
    "    else:\n",
    "        level = 'invalid'\n",
    "    return level\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal_level = udf(salary_level, StringType()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+------------+\n",
      "|emp_id|    name|dept_id|salary|salary_level|\n",
      "+------+--------+-------+------+------------+\n",
      "|     1|   Smith|     fi|  1000|     invalid|\n",
      "|     2|    Rose|     ma|  2000|         low|\n",
      "|     3|Williams|     ma|  7000|        high|\n",
      "|     4|   Jones|     sa|  2000|         low|\n",
      "|     5|   Brown|     sa|  1000|     invalid|\n",
      "|     6|   Katie|     fi|  3500|      medium|\n",
      "|     7|   Linda|     it|  2000|         low|\n",
      "|     8| Michael|     it|  2000|         low|\n",
      "|     9| Johnson|     ma|  1000|     invalid|\n",
      "|    10|     Tom|     fi|  4300|      medium|\n",
      "|    11|    John|     fi| 10000|        high|\n",
      "|    12|     Doe|     ma|  5000|      medium|\n",
      "|    13|     Ben|     sa|  3000|         low|\n",
      "|    14|  Justin|     it|  5000|      medium|\n",
      "|    15|    Lily|     cs|   100|     invalid|\n",
      "|  NULL|    NULL|   NULL|  1000|     invalid|\n",
      "|    17|    Jane|   NULL|  9000|        high|\n",
      "|    18|    Lisa|   NULL|  2500|         low|\n",
      "|    19|    Lucy|   NULL|  5000|      medium|\n",
      "|    20|    Kate|   NULL|  3000|         low|\n",
      "+------+--------+-------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.withColumn(\"salary_level\", sal_level(df.salary))\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# isNull()\n",
    "newdf = df.filter(df.salary.isNull())\n",
    "newdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# isNotNull()\n",
    "df.filter(df.salary.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- dept_id: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+\n",
      "|emp_id|    name|dept_id|salary|\n",
      "+------+--------+-------+------+\n",
      "|     1|   Smith|     fi|  1000|\n",
      "|     2|    Rose|     ma|  2000|\n",
      "|     3|Williams|     ma|  7000|\n",
      "|     4|   Jones|     sa|  2000|\n",
      "|     5|   Brown|     sa|  1000|\n",
      "|     6|   Katie|     fi|  3500|\n",
      "|     7|   Linda|     it|  2000|\n",
      "|     8| Michael|     it|  2000|\n",
      "|     9| Johnson|     ma|  1000|\n",
      "|    10|     Tom|     fi|  4300|\n",
      "|    11|    John|     fi| 10000|\n",
      "|    12|     Doe|     ma|  5000|\n",
      "|    13|     Ben|     sa|  3000|\n",
      "|    14|  Justin|     it|  5000|\n",
      "|    15|    Lily|     cs|   100|\n",
      "|  NULL|    NULL|INVALID|  1000|\n",
      "|    17|    Jane|INVALID|  9000|\n",
      "|    18|    Lisa|INVALID|  2500|\n",
      "|    19|    Lucy|INVALID|  5000|\n",
      "|    20|    Kate|INVALID|  3000|\n",
      "+------+--------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fillna()\n",
    "newdf = df.fillna(\"INVALID\", [\"dept_id\"])\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+\n",
      "|emp_id|    name|dept_id|salary|\n",
      "+------+--------+-------+------+\n",
      "|     1|   Smith|     fi|  1000|\n",
      "|     2|    Rose|     ma|  2000|\n",
      "|     3|Williams|     ma|  7000|\n",
      "|     4|   Jones|     sa|  2000|\n",
      "|     5|   Brown|     sa|  1000|\n",
      "|     6|   Katie|     fi|  3500|\n",
      "|     7|   Linda|     it|  2000|\n",
      "|     8| Michael|     it|  2000|\n",
      "|     9| Johnson|     ma|  1000|\n",
      "|    10|     Tom|     fi|  4300|\n",
      "|    11|    John|     fi| 10000|\n",
      "|    12|     Doe|     ma|  5000|\n",
      "|    13|     Ben|     sa|  3000|\n",
      "|    14|  Justin|     it|  5000|\n",
      "|    15|    Lily|     cs|   100|\n",
      "+------+--------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dropna()\n",
    "newdf = df.dropna()\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+\n",
      "|emp_id|    name|dept_id|salary|\n",
      "+------+--------+-------+------+\n",
      "|     1|   Smith|     fi|  1000|\n",
      "|     2|    Rose|     ma|  2000|\n",
      "|     3|Williams|     ma|  7000|\n",
      "|     4|   Jones|     sa|  2000|\n",
      "|     5|   Brown|     sa|  1000|\n",
      "|     6|   Katie|     fi|  3500|\n",
      "|     7|   Linda|     it|  2000|\n",
      "|     8| Michael|     it|  2000|\n",
      "|     9| Johnson|     ma|  1000|\n",
      "|    10|     Tom|     fi|  4300|\n",
      "|    11|    John|     fi| 10000|\n",
      "|    12|     Doe|     ma|  5000|\n",
      "|    13|     Ben|     sa|  3000|\n",
      "|    14|  Justin|     it|  5000|\n",
      "|    15|    Lily|     cs|   100|\n",
      "|  NULL|    NULL|   NULL|  1000|\n",
      "|    17|    Jane|   NULL|  9000|\n",
      "|    18|    Lisa|   NULL|  2500|\n",
      "|    19|    Lucy|   NULL|  5000|\n",
      "|    20|    Kate|   NULL|  3000|\n",
      "+------+--------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.dropna(how='all') # if all values are NA drop that row or column\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+\n",
      "|emp_id|    name|dept_id|salary|\n",
      "+------+--------+-------+------+\n",
      "|     1|   Smith|     fi|  1000|\n",
      "|     2|    Rose|     ma|  2000|\n",
      "|     3|Williams|     ma|  7000|\n",
      "|     4|   Jones|     sa|  2000|\n",
      "|     5|   Brown|     sa|  1000|\n",
      "|     6|   Katie|     fi|  3500|\n",
      "|     7|   Linda|     it|  2000|\n",
      "|     8| Michael|     it|  2000|\n",
      "|     9| Johnson|     ma|  1000|\n",
      "|    10|     Tom|     fi|  4300|\n",
      "|    11|    John|     fi| 10000|\n",
      "|    12|     Doe|     ma|  5000|\n",
      "|    13|     Ben|     sa|  3000|\n",
      "|    14|  Justin|     it|  5000|\n",
      "|    15|    Lily|     cs|   100|\n",
      "+------+--------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove all reows where dept_id is null\n",
    "newdf = df.dropna(subset=\"dept_id\")\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+\n",
      "|emp_id|    name|dept_id|salary|\n",
      "+------+--------+-------+------+\n",
      "|     1|   Smith|     fi|  1000|\n",
      "|     2|    Rose|     ma|  2000|\n",
      "|     3|Williams|     ma|  7000|\n",
      "|     4|   Jones|     sa|  2000|\n",
      "|     5|   Brown|     sa|  1000|\n",
      "|     6|   Katie|     fi|  3500|\n",
      "|     7|   Linda|     it|  2000|\n",
      "|     8| Michael|     it|  2000|\n",
      "|     9| Johnson|     ma|  1000|\n",
      "|    10|     Tom|     fi|  4300|\n",
      "|    11|    John|     fi| 10000|\n",
      "|    12|     Doe|     ma|  5000|\n",
      "|    13|     Ben|     sa|  3000|\n",
      "|    14|  Justin|     it|  5000|\n",
      "|    15|    Lily|     cs|   100|\n",
      "|  NULL|    NULL|   NULL|  1000|\n",
      "|    17|    Jane|   NULL|  9000|\n",
      "|    18|    Lisa|   NULL|  2500|\n",
      "|    19|    Lucy|   NULL|  5000|\n",
      "|    20|    Kate|   NULL|  3000|\n",
      "+------+--------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf = df.repartition(15)\n",
    "newdf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**this is expensive operation as it requires DATA SHUFFLING between workers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coalesce (only decrease nums of partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf = df.coalesce(5)\n",
    "newdf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By default, number of parittions for Spark SQL is 200\n",
    "- Can also set num of parttitions at the Spark application level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 500\n"
     ]
    }
   ],
   "source": [
    "# set num of partitions as Spark Application\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"500\")\n",
    "\n",
    "num_part = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "print(\"Number of Partitions: {0}\".format(num_part))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Catalog\n",
    "This is a user-facing API, which can access via SparkSession.catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **listDatabases()**: return all databases along with their location on file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', catalog='spark_catalog', description='default database', locationUri='file:/F:/Data_Engineering/Apache_Spark/BigDataSpark/3_Advanced_Features/spark-warehouse')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **listTables()**: return all tables for a given database along with info such as the table type (foreign/managed) and whether a partitcular table is temporary or permanent -> this includes all temp views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='hive_dept', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='hive_emp', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='deptdf', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='empdf', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **listColumns()**: return all columns from a particular table in Database (return data types, whether column used in partitions or pools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\spark\\python\\pyspark\\sql\\catalog.py:651: FutureWarning: `dbName` has been deprecated since Spark 3.4 and might be removed in a future version. Use listColumns(`dbName.tableName`) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Column(name='emp_id', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='dept_id', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='salary', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns(\"hive_emp\", \"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **listFunctions()**: return all features available in SparkSession along with the info it is temporary or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Function(name='!', catalog=None, namespace=None, description='! expr - Logical not.', className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True),\n",
       " Function(name='!=', catalog=None, namespace=None, description='expr1 != expr2 - Returns true if `expr1` is not equal to `expr2`.', className=None, isTemporary=True),\n",
       " Function(name='%', catalog=None, namespace=None, description='expr1 % expr2 - Returns the remainder after `expr1`/`expr2`.', className='org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True),\n",
       " Function(name='&', catalog=None, namespace=None, description='expr1 & expr2 - Returns the result of bitwise AND of `expr1` and `expr2`.', className='org.apache.spark.sql.catalyst.expressions.BitwiseAnd', isTemporary=True),\n",
       " Function(name='*', catalog=None, namespace=None, description='expr1 * expr2 - Returns `expr1`*`expr2`.', className='org.apache.spark.sql.catalyst.expressions.Multiply', isTemporary=True),\n",
       " Function(name='+', catalog=None, namespace=None, description='expr1 + expr2 - Returns `expr1`+`expr2`.', className='org.apache.spark.sql.catalyst.expressions.Add', isTemporary=True),\n",
       " Function(name='-', catalog=None, namespace=None, description='expr1 - expr2 - Returns `expr1`-`expr2`.', className='org.apache.spark.sql.catalyst.expressions.Subtract', isTemporary=True),\n",
       " Function(name='/', catalog=None, namespace=None, description='expr1 / expr2 - Returns `expr1`/`expr2`. It always performs floating point division.', className='org.apache.spark.sql.catalyst.expressions.Divide', isTemporary=True),\n",
       " Function(name='<', catalog=None, namespace=None, description='expr1 < expr2 - Returns true if `expr1` is less than `expr2`.', className='org.apache.spark.sql.catalyst.expressions.LessThan', isTemporary=True),\n",
       " Function(name='<=', catalog=None, namespace=None, description='expr1 <= expr2 - Returns true if `expr1` is less than or equal to `expr2`.', className='org.apache.spark.sql.catalyst.expressions.LessThanOrEqual', isTemporary=True),\n",
       " Function(name='<=>', catalog=None, namespace=None, description='\\n    expr1 <=> expr2 - Returns same result as the EQUAL(=) operator for non-null operands,\\n      but returns true if both are null, false if one of the them is null.\\n  ', className='org.apache.spark.sql.catalyst.expressions.EqualNullSafe', isTemporary=True),\n",
       " Function(name='<>', catalog=None, namespace=None, description='expr1 <> expr2 - Returns true if `expr1` is not equal to `expr2`.', className=None, isTemporary=True),\n",
       " Function(name='=', catalog=None, namespace=None, description='expr1 = expr2 - Returns true if `expr1` equals `expr2`, or false otherwise.', className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True),\n",
       " Function(name='==', catalog=None, namespace=None, description='expr1 == expr2 - Returns true if `expr1` equals `expr2`, or false otherwise.', className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True),\n",
       " Function(name='>', catalog=None, namespace=None, description='expr1 > expr2 - Returns true if `expr1` is greater than `expr2`.', className='org.apache.spark.sql.catalyst.expressions.GreaterThan', isTemporary=True),\n",
       " Function(name='>=', catalog=None, namespace=None, description='expr1 >= expr2 - Returns true if `expr1` is greater than or equal to `expr2`.', className='org.apache.spark.sql.catalyst.expressions.GreaterThanOrEqual', isTemporary=True),\n",
       " Function(name='^', catalog=None, namespace=None, description='expr1 ^ expr2 - Returns the result of bitwise exclusive OR of `expr1` and `expr2`.', className='org.apache.spark.sql.catalyst.expressions.BitwiseXor', isTemporary=True),\n",
       " Function(name='abs', catalog=None, namespace=None, description='abs(expr) - Returns the absolute value of the numeric or interval value.', className='org.apache.spark.sql.catalyst.expressions.Abs', isTemporary=True),\n",
       " Function(name='acos', catalog=None, namespace=None, description='\\n    acos(expr) - Returns the inverse cosine (a.k.a. arc cosine) of `expr`, as if computed by\\n      `java.lang.Math.acos`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Acos', isTemporary=True),\n",
       " Function(name='acosh', catalog=None, namespace=None, description='\\n    acosh(expr) - Returns inverse hyperbolic cosine of `expr`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Acosh', isTemporary=True),\n",
       " Function(name='add_months', catalog=None, namespace=None, description='add_months(start_date, num_months) - Returns the date that is `num_months` after `start_date`.', className='org.apache.spark.sql.catalyst.expressions.AddMonths', isTemporary=True),\n",
       " Function(name='aes_decrypt', catalog=None, namespace=None, description=\"\\n    aes_decrypt(expr, key[, mode[, padding[, aad]]]) - Returns a decrypted value of `expr` using AES in `mode` with `padding`.\\n      Key lengths of 16, 24 and 32 bits are supported. Supported combinations of (`mode`, `padding`) are ('ECB', 'PKCS'), ('GCM', 'NONE') and ('CBC', 'PKCS').\\n      Optional additional authenticated data (AAD) is only supported for GCM. If provided for encryption, the identical AAD value must be provided for decryption.\\n      The default mode is GCM.\\n  \", className='org.apache.spark.sql.catalyst.expressions.AesDecrypt', isTemporary=True),\n",
       " Function(name='aes_encrypt', catalog=None, namespace=None, description=\"\\n    aes_encrypt(expr, key[, mode[, padding[, iv[, aad]]]]) - Returns an encrypted value of `expr` using AES in given `mode` with the specified `padding`.\\n      Key lengths of 16, 24 and 32 bits are supported. Supported combinations of (`mode`, `padding`) are ('ECB', 'PKCS'), ('GCM', 'NONE') and ('CBC', 'PKCS').\\n      Optional initialization vectors (IVs) are only supported for CBC and GCM modes. These must be 16 bytes for CBC and 12 bytes for GCM. If not provided, a random vector will be generated and prepended to the output.\\n      Optional additional authenticated data (AAD) is only supported for GCM. If provided for encryption, the identical AAD value must be provided for decryption.\\n      The default mode is GCM.\\n  \", className='org.apache.spark.sql.catalyst.expressions.AesEncrypt', isTemporary=True),\n",
       " Function(name='aggregate', catalog=None, namespace=None, description='\\n      aggregate(expr, start, merge, finish) - Applies a binary operator to an initial state and all\\n      elements in the array, and reduces this to a single state. The final state is converted\\n      into the final result by applying a finish function.\\n    ', className='org.apache.spark.sql.catalyst.expressions.ArrayAggregate', isTemporary=True),\n",
       " Function(name='and', catalog=None, namespace=None, description='expr1 and expr2 - Logical AND.', className='org.apache.spark.sql.catalyst.expressions.And', isTemporary=True),\n",
       " Function(name='any', catalog=None, namespace=None, description='any(expr) - Returns true if at least one value of `expr` is true.', className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
       " Function(name='any_value', catalog=None, namespace=None, description='\\n    any_value(expr[, isIgnoreNull]) - Returns some value of `expr` for a group of rows.\\n      If `isIgnoreNull` is true, returns only non-null values.', className='org.apache.spark.sql.catalyst.expressions.aggregate.AnyValue', isTemporary=True),\n",
       " Function(name='approx_count_distinct', catalog=None, namespace=None, description='\\n    approx_count_distinct(expr[, relativeSD]) - Returns the estimated cardinality by HyperLogLog++.\\n      `relativeSD` defines the maximum relative standard deviation allowed.', className='org.apache.spark.sql.catalyst.expressions.aggregate.HyperLogLogPlusPlus', isTemporary=True),\n",
       " Function(name='approx_percentile', catalog=None, namespace=None, description='\\n    approx_percentile(col, percentage [, accuracy]) - Returns the approximate `percentile` of the numeric or\\n      ansi interval column `col` which is the smallest value in the ordered `col` values (sorted\\n      from least to greatest) such that no more than `percentage` of `col` values is less than\\n      the value or equal to that value. The value of percentage must be between 0.0 and 1.0.\\n      The `accuracy` parameter (default: 10000) is a positive numeric literal which controls\\n      approximation accuracy at the cost of memory. Higher value of `accuracy` yields better\\n      accuracy, `1.0/accuracy` is the relative error of the approximation.\\n      When `percentage` is an array, each value of the percentage array must be between 0.0 and 1.0.\\n      In this case, returns the approximate percentile array of column `col` at the given\\n      percentage array.\\n  ', className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True),\n",
       " Function(name='array', catalog=None, namespace=None, description='array(expr, ...) - Returns an array with the given elements.', className='org.apache.spark.sql.catalyst.expressions.CreateArray', isTemporary=True),\n",
       " Function(name='array_agg', catalog=None, namespace=None, description='array_agg(expr) - Collects and returns a list of non-unique elements.', className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectList', isTemporary=True),\n",
       " Function(name='array_append', catalog=None, namespace=None, description='\\n      array_append(array, element) - Add the element at the end of the array passed as first\\n      argument. Type of element should be similar to type of the elements of the array.\\n      Null element is also appended into the array. But if the array passed, is NULL\\n      output is NULL\\n      ', className='org.apache.spark.sql.catalyst.expressions.ArrayAppend', isTemporary=True),\n",
       " Function(name='array_compact', catalog=None, namespace=None, description='array_compact(array) - Removes null values from the array.', className='org.apache.spark.sql.catalyst.expressions.ArrayCompact', isTemporary=True),\n",
       " Function(name='array_contains', catalog=None, namespace=None, description='array_contains(array, value) - Returns true if the array contains the value.', className='org.apache.spark.sql.catalyst.expressions.ArrayContains', isTemporary=True),\n",
       " Function(name='array_distinct', catalog=None, namespace=None, description='array_distinct(array) - Removes duplicate values from the array.', className='org.apache.spark.sql.catalyst.expressions.ArrayDistinct', isTemporary=True),\n",
       " Function(name='array_except', catalog=None, namespace=None, description='\\n  array_except(array1, array2) - Returns an array of the elements in array1 but not in array2,\\n    without duplicates.\\n  ', className='org.apache.spark.sql.catalyst.expressions.ArrayExcept', isTemporary=True),\n",
       " Function(name='array_insert', catalog=None, namespace=None, description=\"\\n    array_insert(x, pos, val) - Places val into index pos of array x.\\n      Array indices start at 1. The maximum negative index is -1 for which the function inserts\\n      new element after the current last element.\\n      Index above array size appends the array, or prepends the array if index is negative,\\n      with 'null' elements.\\n  \", className='org.apache.spark.sql.catalyst.expressions.ArrayInsert', isTemporary=True),\n",
       " Function(name='array_intersect', catalog=None, namespace=None, description='\\n  array_intersect(array1, array2) - Returns an array of the elements in the intersection of array1 and\\n    array2, without duplicates.\\n  ', className='org.apache.spark.sql.catalyst.expressions.ArrayIntersect', isTemporary=True),\n",
       " Function(name='array_join', catalog=None, namespace=None, description='\\n    array_join(array, delimiter[, nullReplacement]) - Concatenates the elements of the given array\\n      using the delimiter and an optional string to replace nulls. If no value is set for\\n      nullReplacement, any null value is filtered.', className='org.apache.spark.sql.catalyst.expressions.ArrayJoin', isTemporary=True),\n",
       " Function(name='array_max', catalog=None, namespace=None, description='\\n    array_max(array) - Returns the maximum value in the array. NaN is greater than\\n    any non-NaN elements for double/float type. NULL elements are skipped.', className='org.apache.spark.sql.catalyst.expressions.ArrayMax', isTemporary=True),\n",
       " Function(name='array_min', catalog=None, namespace=None, description='\\n    array_min(array) - Returns the minimum value in the array. NaN is greater than\\n    any non-NaN elements for double/float type. NULL elements are skipped.', className='org.apache.spark.sql.catalyst.expressions.ArrayMin', isTemporary=True),\n",
       " Function(name='array_position', catalog=None, namespace=None, description='\\n    array_position(array, element) - Returns the (1-based) index of the first matching element of\\n      the array as long, or 0 if no match is found.\\n  ', className='org.apache.spark.sql.catalyst.expressions.ArrayPosition', isTemporary=True),\n",
       " Function(name='array_prepend', catalog=None, namespace=None, description='\\n      array_prepend(array, element) - Add the element at the beginning of the array passed as first\\n      argument. Type of element should be the same as the type of the elements of the array.\\n      Null element is also prepended to the array. But if the array passed is NULL\\n      output is NULL\\n    ', className='org.apache.spark.sql.catalyst.expressions.ArrayPrepend', isTemporary=True),\n",
       " Function(name='array_remove', catalog=None, namespace=None, description='array_remove(array, element) - Remove all elements that equal to element from array.', className='org.apache.spark.sql.catalyst.expressions.ArrayRemove', isTemporary=True),\n",
       " Function(name='array_repeat', catalog=None, namespace=None, description='array_repeat(element, count) - Returns the array containing element count times.', className='org.apache.spark.sql.catalyst.expressions.ArrayRepeat', isTemporary=True),\n",
       " Function(name='array_size', catalog=None, namespace=None, description='array_size(expr) - Returns the size of an array. The function returns null for null input.', className='org.apache.spark.sql.catalyst.expressions.ArraySize', isTemporary=True),\n",
       " Function(name='array_sort', catalog=None, namespace=None, description='array_sort(expr, func) - Sorts the input array. If func is omitted, sort\\n    in ascending order. The elements of the input array must be orderable.\\n    NaN is greater than any non-NaN elements for double/float type.\\n    Null elements will be placed at the end of the returned array.\\n    Since 3.0.0 this function also sorts and returns the array based on the\\n    given comparator function. The comparator will take two arguments representing\\n    two elements of the array.\\n    It returns a negative integer, 0, or a positive integer as the first element is less than,\\n    equal to, or greater than the second element. If the comparator function returns null,\\n    the function will fail and raise an error.\\n    ', className='org.apache.spark.sql.catalyst.expressions.ArraySort', isTemporary=True),\n",
       " Function(name='array_union', catalog=None, namespace=None, description='\\n    array_union(array1, array2) - Returns an array of the elements in the union of array1 and array2,\\n      without duplicates.\\n  ', className='org.apache.spark.sql.catalyst.expressions.ArrayUnion', isTemporary=True),\n",
       " Function(name='arrays_overlap', catalog=None, namespace=None, description='arrays_overlap(a1, a2) - Returns true if a1 contains at least a non-null element present also in a2. If the arrays have no common element and they are both non-empty and either of them contains a null element null is returned, false otherwise.', className='org.apache.spark.sql.catalyst.expressions.ArraysOverlap', isTemporary=True),\n",
       " Function(name='arrays_zip', catalog=None, namespace=None, description='\\n    arrays_zip(a1, a2, ...) - Returns a merged array of structs in which the N-th struct contains all\\n    N-th values of input arrays.\\n  ', className='org.apache.spark.sql.catalyst.expressions.ArraysZip', isTemporary=True),\n",
       " Function(name='ascii', catalog=None, namespace=None, description='ascii(str) - Returns the numeric value of the first character of `str`.', className='org.apache.spark.sql.catalyst.expressions.Ascii', isTemporary=True),\n",
       " Function(name='asin', catalog=None, namespace=None, description='\\n    asin(expr) - Returns the inverse sine (a.k.a. arc sine) the arc sin of `expr`,\\n      as if computed by `java.lang.Math.asin`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Asin', isTemporary=True),\n",
       " Function(name='asinh', catalog=None, namespace=None, description='\\n    asinh(expr) - Returns inverse hyperbolic sine of `expr`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Asinh', isTemporary=True),\n",
       " Function(name='assert_true', catalog=None, namespace=None, description='assert_true(expr) - Throws an exception if `expr` is not true.', className='org.apache.spark.sql.catalyst.expressions.AssertTrue', isTemporary=True),\n",
       " Function(name='atan', catalog=None, namespace=None, description='\\n    atan(expr) - Returns the inverse tangent (a.k.a. arc tangent) of `expr`, as if computed by\\n      `java.lang.Math.atan`\\n  ', className='org.apache.spark.sql.catalyst.expressions.Atan', isTemporary=True),\n",
       " Function(name='atan2', catalog=None, namespace=None, description='\\n    atan2(exprY, exprX) - Returns the angle in radians between the positive x-axis of a plane\\n      and the point given by the coordinates (`exprX`, `exprY`), as if computed by\\n      `java.lang.Math.atan2`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Atan2', isTemporary=True),\n",
       " Function(name='atanh', catalog=None, namespace=None, description='\\n    atanh(expr) - Returns inverse hyperbolic tangent of `expr`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Atanh', isTemporary=True),\n",
       " Function(name='avg', catalog=None, namespace=None, description='avg(expr) - Returns the mean calculated from values of a group.', className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True),\n",
       " Function(name='base64', catalog=None, namespace=None, description='base64(bin) - Converts the argument from a binary `bin` to a base 64 string.', className='org.apache.spark.sql.catalyst.expressions.Base64', isTemporary=True),\n",
       " Function(name='between', catalog=None, namespace=None, description='expr1 [NOT] BETWEEN expr2 AND expr3 - evaluate if `expr1` is [not] in between `expr2` and `expr3`.', className=None, isTemporary=True),\n",
       " Function(name='bigint', catalog=None, namespace=None, description='bigint(expr) - Casts the value `expr` to the target data type `bigint`.', className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='bin', catalog=None, namespace=None, description='bin(expr) - Returns the string representation of the long value `expr` represented in binary.', className='org.apache.spark.sql.catalyst.expressions.Bin', isTemporary=True),\n",
       " Function(name='binary', catalog=None, namespace=None, description='binary(expr) - Casts the value `expr` to the target data type `binary`.', className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='bit_and', catalog=None, namespace=None, description='bit_and(expr) - Returns the bitwise AND of all non-null input values, or null if none.', className='org.apache.spark.sql.catalyst.expressions.aggregate.BitAndAgg', isTemporary=True),\n",
       " Function(name='bit_count', catalog=None, namespace=None, description='bit_count(expr) - Returns the number of bits that are set in the argument expr as an unsigned 64-bit integer, or NULL if the argument is NULL.', className='org.apache.spark.sql.catalyst.expressions.BitwiseCount', isTemporary=True),\n",
       " Function(name='bit_get', catalog=None, namespace=None, description='\\n    bit_get(expr, pos) - Returns the value of the bit (0 or 1) at the specified position.\\n      The positions are numbered from right to left, starting at zero.\\n      The position argument cannot be negative.\\n  ', className='org.apache.spark.sql.catalyst.expressions.BitwiseGet', isTemporary=True),\n",
       " Function(name='bit_length', catalog=None, namespace=None, description='bit_length(expr) - Returns the bit length of string data or number of bits of binary data.', className='org.apache.spark.sql.catalyst.expressions.BitLength', isTemporary=True),\n",
       " Function(name='bit_or', catalog=None, namespace=None, description='bit_or(expr) - Returns the bitwise OR of all non-null input values, or null if none.', className='org.apache.spark.sql.catalyst.expressions.aggregate.BitOrAgg', isTemporary=True),\n",
       " Function(name='bit_xor', catalog=None, namespace=None, description='bit_xor(expr) - Returns the bitwise XOR of all non-null input values, or null if none.', className='org.apache.spark.sql.catalyst.expressions.aggregate.BitXorAgg', isTemporary=True),\n",
       " Function(name='bitmap_bit_position', catalog=None, namespace=None, description='bitmap_bit_position(child) - Returns the bit position for the given input child expression.', className='org.apache.spark.sql.catalyst.expressions.BitmapBitPosition', isTemporary=True),\n",
       " Function(name='bitmap_bucket_number', catalog=None, namespace=None, description='bitmap_bucket_number(child) - Returns the bucket number for the given input child expression.', className='org.apache.spark.sql.catalyst.expressions.BitmapBucketNumber', isTemporary=True),\n",
       " Function(name='bitmap_construct_agg', catalog=None, namespace=None, description='\\n    bitmap_construct_agg(child) - Returns a bitmap with the positions of the bits set from all the values from\\n    the child expression. The child expression will most likely be bitmap_bit_position().\\n  ', className='org.apache.spark.sql.catalyst.expressions.BitmapConstructAgg', isTemporary=True),\n",
       " Function(name='bitmap_count', catalog=None, namespace=None, description='bitmap_count(child) - Returns the number of set bits in the child bitmap.', className='org.apache.spark.sql.catalyst.expressions.BitmapCount', isTemporary=True),\n",
       " Function(name='bitmap_or_agg', catalog=None, namespace=None, description='\\n    bitmap_or_agg(child) - Returns a bitmap that is the bitwise OR of all of the bitmaps from the child\\n    expression. The input should be bitmaps created from bitmap_construct_agg().\\n  ', className='org.apache.spark.sql.catalyst.expressions.BitmapOrAgg', isTemporary=True),\n",
       " Function(name='bool_and', catalog=None, namespace=None, description='bool_and(expr) - Returns true if all values of `expr` are true.', className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolAnd', isTemporary=True),\n",
       " Function(name='bool_or', catalog=None, namespace=None, description='bool_or(expr) - Returns true if at least one value of `expr` is true.', className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
       " Function(name='boolean', catalog=None, namespace=None, description='boolean(expr) - Casts the value `expr` to the target data type `boolean`.', className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='bround', catalog=None, namespace=None, description='bround(expr, d) - Returns `expr` rounded to `d` decimal places using HALF_EVEN rounding mode.', className='org.apache.spark.sql.catalyst.expressions.BRound', isTemporary=True),\n",
       " Function(name='btrim', catalog=None, namespace=None, description='\\n    btrim(str) - Removes the leading and trailing space characters from `str`.\\n\\n    btrim(str, trimStr) - Remove the leading and trailing `trimStr` characters from `str`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.StringTrimBoth', isTemporary=True),\n",
       " Function(name='cardinality', catalog=None, namespace=None, description='\\n    cardinality(expr) - Returns the size of an array or a map.\\n    The function returns null for null input if spark.sql.legacy.sizeOfNull is set to false or\\n    spark.sql.ansi.enabled is set to true. Otherwise, the function returns -1 for null input.\\n    With the default settings, the function returns -1 for null input.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True),\n",
       " Function(name='case', catalog=None, namespace=None, description='CASE expr1 WHEN expr2 THEN expr3 [WHEN expr4 THEN expr5]* [ELSE expr6] END - When `expr1` = `expr2`, returns `expr3`; when `expr1` = `expr4`, return `expr5`; else return `expr6`.', className=None, isTemporary=True),\n",
       " Function(name='cast', catalog=None, namespace=None, description='cast(expr AS type) - Casts the value `expr` to the target data type `type`.', className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='cbrt', catalog=None, namespace=None, description='cbrt(expr) - Returns the cube root of `expr`.', className='org.apache.spark.sql.catalyst.expressions.Cbrt', isTemporary=True),\n",
       " Function(name='ceil', catalog=None, namespace=None, description='ceil(expr[, scale]) - Returns the smallest number after rounding up that is not smaller than `expr`. An optional `scale` parameter can be specified to control the rounding behavior.', className='org.apache.spark.sql.catalyst.expressions.CeilExpressionBuilder', isTemporary=True),\n",
       " Function(name='ceiling', catalog=None, namespace=None, description='ceiling(expr[, scale]) - Returns the smallest number after rounding up that is not smaller than `expr`. An optional `scale` parameter can be specified to control the rounding behavior.', className='org.apache.spark.sql.catalyst.expressions.CeilExpressionBuilder', isTemporary=True),\n",
       " Function(name='char', catalog=None, namespace=None, description='char(expr) - Returns the ASCII character having the binary equivalent to `expr`. If n is larger than 256 the result is equivalent to chr(n % 256)', className='org.apache.spark.sql.catalyst.expressions.Chr', isTemporary=True),\n",
       " Function(name='char_length', catalog=None, namespace=None, description='char_length(expr) - Returns the character length of string data or number of bytes of binary data. The length of string data includes the trailing spaces. The length of binary data includes binary zeros.', className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
       " Function(name='character_length', catalog=None, namespace=None, description='character_length(expr) - Returns the character length of string data or number of bytes of binary data. The length of string data includes the trailing spaces. The length of binary data includes binary zeros.', className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
       " Function(name='chr', catalog=None, namespace=None, description='chr(expr) - Returns the ASCII character having the binary equivalent to `expr`. If n is larger than 256 the result is equivalent to chr(n % 256)', className='org.apache.spark.sql.catalyst.expressions.Chr', isTemporary=True),\n",
       " Function(name='coalesce', catalog=None, namespace=None, description='coalesce(expr1, expr2, ...) - Returns the first non-null argument if exists. Otherwise, null.', className='org.apache.spark.sql.catalyst.expressions.Coalesce', isTemporary=True),\n",
       " Function(name='collect_list', catalog=None, namespace=None, description='collect_list(expr) - Collects and returns a list of non-unique elements.', className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectList', isTemporary=True),\n",
       " Function(name='collect_set', catalog=None, namespace=None, description='collect_set(expr) - Collects and returns a set of unique elements.', className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectSet', isTemporary=True),\n",
       " Function(name='concat', catalog=None, namespace=None, description='concat(col1, col2, ..., colN) - Returns the concatenation of col1, col2, ..., colN.', className='org.apache.spark.sql.catalyst.expressions.Concat', isTemporary=True),\n",
       " Function(name='concat_ws', catalog=None, namespace=None, description='concat_ws(sep[, str | array(str)]+) - Returns the concatenation of the strings separated by `sep`, skipping null values.', className='org.apache.spark.sql.catalyst.expressions.ConcatWs', isTemporary=True),\n",
       " Function(name='contains', catalog=None, namespace=None, description='\\n    contains(left, right) - Returns a boolean. The value is True if right is found inside left.\\n    Returns NULL if either input expression is NULL. Otherwise, returns False.\\n    Both left or right must be of STRING or BINARY type.\\n  ', className='org.apache.spark.sql.catalyst.expressions.ContainsExpressionBuilder', isTemporary=True),\n",
       " Function(name='conv', catalog=None, namespace=None, description='conv(num, from_base, to_base) - Convert `num` from `from_base` to `to_base`.', className='org.apache.spark.sql.catalyst.expressions.Conv', isTemporary=True),\n",
       " Function(name='convert_timezone', catalog=None, namespace=None, description='convert_timezone([sourceTz, ]targetTz, sourceTs) - Converts the timestamp without time zone `sourceTs` from the `sourceTz` time zone to `targetTz`. ', className='org.apache.spark.sql.catalyst.expressions.ConvertTimezone', isTemporary=True),\n",
       " Function(name='corr', catalog=None, namespace=None, description='corr(expr1, expr2) - Returns Pearson coefficient of correlation between a set of number pairs.', className='org.apache.spark.sql.catalyst.expressions.aggregate.Corr', isTemporary=True),\n",
       " Function(name='cos', catalog=None, namespace=None, description='\\n    cos(expr) - Returns the cosine of `expr`, as if computed by\\n      `java.lang.Math.cos`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Cos', isTemporary=True),\n",
       " Function(name='cosh', catalog=None, namespace=None, description='\\n      cosh(expr) - Returns the hyperbolic cosine of `expr`, as if computed by\\n        `java.lang.Math.cosh`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Cosh', isTemporary=True),\n",
       " Function(name='cot', catalog=None, namespace=None, description='\\n    cot(expr) - Returns the cotangent of `expr`, as if computed by `1/java.lang.Math.tan`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Cot', isTemporary=True),\n",
       " Function(name='count', catalog=None, namespace=None, description='\\n    count(*) - Returns the total number of retrieved rows, including rows containing null.\\n\\n    count(expr[, expr...]) - Returns the number of rows for which the supplied expression(s) are all non-null.\\n\\n    count(DISTINCT expr[, expr...]) - Returns the number of rows for which the supplied expression(s) are unique and non-null.\\n  ', className='org.apache.spark.sql.catalyst.expressions.aggregate.Count', isTemporary=True),\n",
       " Function(name='count_if', catalog=None, namespace=None, description='\\n    count_if(expr) - Returns the number of `TRUE` values for the expression.\\n  ', className='org.apache.spark.sql.catalyst.expressions.aggregate.CountIf', isTemporary=True),\n",
       " Function(name='count_min_sketch', catalog=None, namespace=None, description='\\n    count_min_sketch(col, eps, confidence, seed) - Returns a count-min sketch of a column with the given esp,\\n      confidence and seed. The result is an array of bytes, which can be deserialized to a\\n      `CountMinSketch` before usage. Count-min sketch is a probabilistic data structure used for\\n      cardinality estimation using sub-linear space.\\n  ', className='org.apache.spark.sql.catalyst.expressions.aggregate.CountMinSketchAggExpressionBuilder', isTemporary=True),\n",
       " Function(name='covar_pop', catalog=None, namespace=None, description='covar_pop(expr1, expr2) - Returns the population covariance of a set of number pairs.', className='org.apache.spark.sql.catalyst.expressions.aggregate.CovPopulation', isTemporary=True),\n",
       " Function(name='covar_samp', catalog=None, namespace=None, description='covar_samp(expr1, expr2) - Returns the sample covariance of a set of number pairs.', className='org.apache.spark.sql.catalyst.expressions.aggregate.CovSample', isTemporary=True),\n",
       " Function(name='crc32', catalog=None, namespace=None, description='crc32(expr) - Returns a cyclic redundancy check value of the `expr` as a bigint.', className='org.apache.spark.sql.catalyst.expressions.Crc32', isTemporary=True),\n",
       " Function(name='csc', catalog=None, namespace=None, description='\\n    csc(expr) - Returns the cosecant of `expr`, as if computed by `1/java.lang.Math.sin`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Csc', isTemporary=True),\n",
       " Function(name='cume_dist', catalog=None, namespace=None, description='\\n    cume_dist() - Computes the position of a value relative to all values in the partition.\\n  ', className='org.apache.spark.sql.catalyst.expressions.CumeDist', isTemporary=True),\n",
       " Function(name='curdate', catalog=None, namespace=None, description='\\n    curdate() - Returns the current date at the start of query evaluation. All calls of curdate within the same query return the same value.\\n  ', className='org.apache.spark.sql.catalyst.expressions.CurDateExpressionBuilder', isTemporary=True),\n",
       " Function(name='current_catalog', catalog=None, namespace=None, description='current_catalog() - Returns the current catalog.', className='org.apache.spark.sql.catalyst.expressions.CurrentCatalog', isTemporary=True),\n",
       " Function(name='current_database', catalog=None, namespace=None, description='current_database() - Returns the current database.', className='org.apache.spark.sql.catalyst.expressions.CurrentDatabase', isTemporary=True),\n",
       " Function(name='current_date', catalog=None, namespace=None, description='\\n    current_date() - Returns the current date at the start of query evaluation. All calls of current_date within the same query return the same value.\\n\\n    current_date - Returns the current date at the start of query evaluation.\\n  ', className='org.apache.spark.sql.catalyst.expressions.CurrentDate', isTemporary=True),\n",
       " Function(name='current_schema', catalog=None, namespace=None, description='current_schema() - Returns the current database.', className='org.apache.spark.sql.catalyst.expressions.CurrentDatabase', isTemporary=True),\n",
       " Function(name='current_timestamp', catalog=None, namespace=None, description='\\n    current_timestamp() - Returns the current timestamp at the start of query evaluation. All calls of current_timestamp within the same query return the same value.\\n\\n    current_timestamp - Returns the current timestamp at the start of query evaluation.\\n  ', className='org.apache.spark.sql.catalyst.expressions.CurrentTimestamp', isTemporary=True),\n",
       " Function(name='current_timezone', catalog=None, namespace=None, description='current_timezone() - Returns the current session local timezone.', className='org.apache.spark.sql.catalyst.expressions.CurrentTimeZone', isTemporary=True),\n",
       " Function(name='current_user', catalog=None, namespace=None, description='current_user() - user name of current execution context.', className='org.apache.spark.sql.catalyst.expressions.CurrentUser', isTemporary=True),\n",
       " Function(name='date', catalog=None, namespace=None, description='date(expr) - Casts the value `expr` to the target data type `date`.', className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='date_add', catalog=None, namespace=None, description='date_add(start_date, num_days) - Returns the date that is `num_days` after `start_date`.', className='org.apache.spark.sql.catalyst.expressions.DateAdd', isTemporary=True),\n",
       " Function(name='date_diff', catalog=None, namespace=None, description='date_diff(endDate, startDate) - Returns the number of days from `startDate` to `endDate`.', className='org.apache.spark.sql.catalyst.expressions.DateDiff', isTemporary=True),\n",
       " Function(name='date_format', catalog=None, namespace=None, description='date_format(timestamp, fmt) - Converts `timestamp` to a value of string in the format specified by the date format `fmt`.', className='org.apache.spark.sql.catalyst.expressions.DateFormatClass', isTemporary=True),\n",
       " Function(name='date_from_unix_date', catalog=None, namespace=None, description='date_from_unix_date(days) - Create date from the number of days since 1970-01-01.', className='org.apache.spark.sql.catalyst.expressions.DateFromUnixDate', isTemporary=True),\n",
       " Function(name='date_part', catalog=None, namespace=None, description='date_part(field, source) - Extracts a part of the date/timestamp or interval source.', className='org.apache.spark.sql.catalyst.expressions.DatePartExpressionBuilder', isTemporary=True),\n",
       " Function(name='date_sub', catalog=None, namespace=None, description='date_sub(start_date, num_days) - Returns the date that is `num_days` before `start_date`.', className='org.apache.spark.sql.catalyst.expressions.DateSub', isTemporary=True),\n",
       " Function(name='date_trunc', catalog=None, namespace=None, description='\\n    date_trunc(fmt, ts) - Returns timestamp `ts` truncated to the unit specified by the format model `fmt`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.TruncTimestamp', isTemporary=True),\n",
       " Function(name='dateadd', catalog=None, namespace=None, description='dateadd(start_date, num_days) - Returns the date that is `num_days` after `start_date`.', className='org.apache.spark.sql.catalyst.expressions.DateAdd', isTemporary=True),\n",
       " Function(name='datediff', catalog=None, namespace=None, description='datediff(endDate, startDate) - Returns the number of days from `startDate` to `endDate`.', className='org.apache.spark.sql.catalyst.expressions.DateDiff', isTemporary=True),\n",
       " Function(name='datepart', catalog=None, namespace=None, description='datepart(field, source) - Extracts a part of the date/timestamp or interval source.', className='org.apache.spark.sql.catalyst.expressions.DatePartExpressionBuilder', isTemporary=True),\n",
       " Function(name='day', catalog=None, namespace=None, description='day(date) - Returns the day of month of the date/timestamp.', className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True),\n",
       " Function(name='dayofmonth', catalog=None, namespace=None, description='dayofmonth(date) - Returns the day of month of the date/timestamp.', className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True),\n",
       " Function(name='dayofweek', catalog=None, namespace=None, description='dayofweek(date) - Returns the day of the week for date/timestamp (1 = Sunday, 2 = Monday, ..., 7 = Saturday).', className='org.apache.spark.sql.catalyst.expressions.DayOfWeek', isTemporary=True),\n",
       " Function(name='dayofyear', catalog=None, namespace=None, description='dayofyear(date) - Returns the day of year of the date/timestamp.', className='org.apache.spark.sql.catalyst.expressions.DayOfYear', isTemporary=True),\n",
       " Function(name='decimal', catalog=None, namespace=None, description='decimal(expr) - Casts the value `expr` to the target data type `decimal`.', className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='decode', catalog=None, namespace=None, description='\\n    decode(bin, charset) - Decodes the first argument using the second argument character set.\\n\\n    decode(expr, search, result [, search, result ] ... [, default]) - Compares expr\\n      to each search value in order. If expr is equal to a search value, decode returns\\n      the corresponding result. If no match is found, then it returns default. If default\\n      is omitted, it returns null.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Decode', isTemporary=True),\n",
       " Function(name='degrees', catalog=None, namespace=None, description='degrees(expr) - Converts radians to degrees.', className='org.apache.spark.sql.catalyst.expressions.ToDegrees', isTemporary=True),\n",
       " Function(name='dense_rank', catalog=None, namespace=None, description='\\n    dense_rank() - Computes the rank of a value in a group of values. The result is one plus the\\n      previously assigned rank value. Unlike the function rank, dense_rank will not produce gaps\\n      in the ranking sequence.\\n  ', className='org.apache.spark.sql.catalyst.expressions.DenseRank', isTemporary=True),\n",
       " Function(name='div', catalog=None, namespace=None, description='expr1 div expr2 - Divide `expr1` by `expr2`. It returns NULL if an operand is NULL or `expr2` is 0. The result is casted to long.', className='org.apache.spark.sql.catalyst.expressions.IntegralDivide', isTemporary=True),\n",
       " Function(name='double', catalog=None, namespace=None, description='double(expr) - Casts the value `expr` to the target data type `double`.', className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='e', catalog=None, namespace=None, description=\"e() - Returns Euler's number, e.\", className='org.apache.spark.sql.catalyst.expressions.EulerNumber', isTemporary=True),\n",
       " Function(name='element_at', catalog=None, namespace=None, description='\\n    element_at(array, index) - Returns element of array at given (1-based) index. If Index is 0,\\n      Spark will throw an error. If index < 0, accesses elements from the last to the first.\\n      The function returns NULL if the index exceeds the length of the array and\\n      `spark.sql.ansi.enabled` is set to false.\\n      If `spark.sql.ansi.enabled` is set to true, it throws ArrayIndexOutOfBoundsException\\n      for invalid indices.\\n\\n    element_at(map, key) - Returns value for given key. The function returns NULL if the key is not\\n       contained in the map.\\n  ', className='org.apache.spark.sql.catalyst.expressions.ElementAt', isTemporary=True),\n",
       " Function(name='elt', catalog=None, namespace=None, description='\\n    elt(n, input1, input2, ...) - Returns the `n`-th input, e.g., returns `input2` when `n` is 2.\\n    The function returns NULL if the index exceeds the length of the array\\n    and `spark.sql.ansi.enabled` is set to false. If `spark.sql.ansi.enabled` is set to true,\\n    it throws ArrayIndexOutOfBoundsException for invalid indices.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Elt', isTemporary=True),\n",
       " Function(name='encode', catalog=None, namespace=None, description='encode(str, charset) - Encodes the first argument using the second argument character set.', className='org.apache.spark.sql.catalyst.expressions.Encode', isTemporary=True),\n",
       " Function(name='endswith', catalog=None, namespace=None, description='\\n    endswith(left, right) - Returns a boolean. The value is True if left ends with right.\\n    Returns NULL if either input expression is NULL. Otherwise, returns False.\\n    Both left or right must be of STRING or BINARY type.\\n  ', className='org.apache.spark.sql.catalyst.expressions.EndsWithExpressionBuilder', isTemporary=True),\n",
       " Function(name='equal_null', catalog=None, namespace=None, description='\\n    equal_null(expr1, expr2) - Returns same result as the EQUAL(=) operator for non-null operands,\\n      but returns true if both are null, false if one of the them is null.\\n  ', className='org.apache.spark.sql.catalyst.expressions.EqualNull', isTemporary=True),\n",
       " Function(name='every', catalog=None, namespace=None, description='every(expr) - Returns true if all values of `expr` are true.', className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolAnd', isTemporary=True),\n",
       " Function(name='exists', catalog=None, namespace=None, description='exists(expr, pred) - Tests whether a predicate holds for one or more elements in the array.', className='org.apache.spark.sql.catalyst.expressions.ArrayExists', isTemporary=True),\n",
       " Function(name='exp', catalog=None, namespace=None, description='exp(expr) - Returns e to the power of `expr`.', className='org.apache.spark.sql.catalyst.expressions.Exp', isTemporary=True),\n",
       " Function(name='explode', catalog=None, namespace=None, description='explode(expr) - Separates the elements of array `expr` into multiple rows, or the elements of map `expr` into multiple rows and columns. Unless specified otherwise, uses the default column name `col` for elements of the array or `key` and `value` for the elements of the map.', className='org.apache.spark.sql.catalyst.expressions.ExplodeExpressionBuilder', isTemporary=True),\n",
       " Function(name='explode_outer', catalog=None, namespace=None, description='explode_outer(expr) - Separates the elements of array `expr` into multiple rows, or the elements of map `expr` into multiple rows and columns. Unless specified otherwise, uses the default column name `col` for elements of the array or `key` and `value` for the elements of the map.', className='org.apache.spark.sql.catalyst.expressions.ExplodeExpressionBuilder', isTemporary=True),\n",
       " Function(name='expm1', catalog=None, namespace=None, description='expm1(expr) - Returns exp(`expr`) - 1.', className='org.apache.spark.sql.catalyst.expressions.Expm1', isTemporary=True),\n",
       " Function(name='extract', catalog=None, namespace=None, description='extract(field FROM source) - Extracts a part of the date/timestamp or interval source.', className='org.apache.spark.sql.catalyst.expressions.Extract', isTemporary=True),\n",
       " Function(name='factorial', catalog=None, namespace=None, description='factorial(expr) - Returns the factorial of `expr`. `expr` is [0..20]. Otherwise, null.', className='org.apache.spark.sql.catalyst.expressions.Factorial', isTemporary=True),\n",
       " Function(name='filter', catalog=None, namespace=None, description='filter(expr, func) - Filters the input array using the given predicate.', className='org.apache.spark.sql.catalyst.expressions.ArrayFilter', isTemporary=True),\n",
       " Function(name='find_in_set', catalog=None, namespace=None, description='\\n    find_in_set(str, str_array) - Returns the index (1-based) of the given string (`str`) in the comma-delimited list (`str_array`).\\n      Returns 0, if the string was not found or if the given string (`str`) contains a comma.\\n  ', className='org.apache.spark.sql.catalyst.expressions.FindInSet', isTemporary=True),\n",
       " Function(name='first', catalog=None, namespace=None, description='\\n    first(expr[, isIgnoreNull]) - Returns the first value of `expr` for a group of rows.\\n      If `isIgnoreNull` is true, returns only non-null values.', className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True),\n",
       " Function(name='first_value', catalog=None, namespace=None, description='\\n    first_value(expr[, isIgnoreNull]) - Returns the first value of `expr` for a group of rows.\\n      If `isIgnoreNull` is true, returns only non-null values.', className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True),\n",
       " Function(name='flatten', catalog=None, namespace=None, description='flatten(arrayOfArrays) - Transforms an array of arrays into a single array.', className='org.apache.spark.sql.catalyst.expressions.Flatten', isTemporary=True),\n",
       " Function(name='float', catalog=None, namespace=None, description='float(expr) - Casts the value `expr` to the target data type `float`.', className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='floor', catalog=None, namespace=None, description=' floor(expr[, scale]) - Returns the largest number after rounding down that is not greater than `expr`. An optional `scale` parameter can be specified to control the rounding behavior.', className='org.apache.spark.sql.catalyst.expressions.FloorExpressionBuilder', isTemporary=True),\n",
       " Function(name='forall', catalog=None, namespace=None, description='forall(expr, pred) - Tests whether a predicate holds for all elements in the array.', className='org.apache.spark.sql.catalyst.expressions.ArrayForAll', isTemporary=True),\n",
       " Function(name='format_number', catalog=None, namespace=None, description=\"\\n    format_number(expr1, expr2) - Formats the number `expr1` like '#,###,###.##', rounded to `expr2`\\n      decimal places. If `expr2` is 0, the result has no decimal point or fractional part.\\n      `expr2` also accept a user specified format.\\n      This is supposed to function like MySQL's FORMAT.\\n  \", className='org.apache.spark.sql.catalyst.expressions.FormatNumber', isTemporary=True),\n",
       " Function(name='format_string', catalog=None, namespace=None, description='format_string(strfmt, obj, ...) - Returns a formatted string from printf-style format strings.', className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True),\n",
       " Function(name='from_csv', catalog=None, namespace=None, description='from_csv(csvStr, schema[, options]) - Returns a struct value with the given `csvStr` and `schema`.', className='org.apache.spark.sql.catalyst.expressions.CsvToStructs', isTemporary=True),\n",
       " Function(name='from_json', catalog=None, namespace=None, description='from_json(jsonStr, schema[, options]) - Returns a struct value with the given `jsonStr` and `schema`.', className='org.apache.spark.sql.catalyst.expressions.JsonToStructs', isTemporary=True),\n",
       " Function(name='from_unixtime', catalog=None, namespace=None, description='from_unixtime(unix_time[, fmt]) - Returns `unix_time` in the specified `fmt`.', className='org.apache.spark.sql.catalyst.expressions.FromUnixTime', isTemporary=True),\n",
       " Function(name='from_utc_timestamp', catalog=None, namespace=None, description=\"from_utc_timestamp(timestamp, timezone) - Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in UTC, and renders that time as a timestamp in the given time zone. For example, 'GMT+1' would yield '2017-07-14 03:40:00.0'.\", className='org.apache.spark.sql.catalyst.expressions.FromUTCTimestamp', isTemporary=True),\n",
       " Function(name='get', catalog=None, namespace=None, description='\\n    get(array, index) - Returns element of array at given (0-based) index. If the index points\\n     outside of the array boundaries, then this function returns NULL.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Get', isTemporary=True),\n",
       " Function(name='get_json_object', catalog=None, namespace=None, description='get_json_object(json_txt, path) - Extracts a json object from `path`.', className='org.apache.spark.sql.catalyst.expressions.GetJsonObject', isTemporary=True),\n",
       " Function(name='getbit', catalog=None, namespace=None, description='\\n    getbit(expr, pos) - Returns the value of the bit (0 or 1) at the specified position.\\n      The positions are numbered from right to left, starting at zero.\\n      The position argument cannot be negative.\\n  ', className='org.apache.spark.sql.catalyst.expressions.BitwiseGet', isTemporary=True),\n",
       " Function(name='greatest', catalog=None, namespace=None, description='greatest(expr, ...) - Returns the greatest value of all parameters, skipping null values.', className='org.apache.spark.sql.catalyst.expressions.Greatest', isTemporary=True),\n",
       " Function(name='grouping', catalog=None, namespace=None, description='\\n    grouping(col) - indicates whether a specified column in a GROUP BY is aggregated or\\n      not, returns 1 for aggregated or 0 for not aggregated in the result set.\",\\n  ', className='org.apache.spark.sql.catalyst.expressions.Grouping', isTemporary=True),\n",
       " Function(name='grouping_id', catalog=None, namespace=None, description='\\n    grouping_id([col1[, col2 ..]]) - returns the level of grouping, equals to\\n      `(grouping(c1) << (n-1)) + (grouping(c2) << (n-2)) + ... + grouping(cn)`\\n  ', className='org.apache.spark.sql.catalyst.expressions.GroupingID', isTemporary=True),\n",
       " Function(name='hash', catalog=None, namespace=None, description='hash(expr1, expr2, ...) - Returns a hash value of the arguments.', className='org.apache.spark.sql.catalyst.expressions.Murmur3Hash', isTemporary=True),\n",
       " Function(name='hex', catalog=None, namespace=None, description='hex(expr) - Converts `expr` to hexadecimal.', className='org.apache.spark.sql.catalyst.expressions.Hex', isTemporary=True),\n",
       " Function(name='histogram_numeric', catalog=None, namespace=None, description=\"\\n    histogram_numeric(expr, nb) - Computes a histogram on numeric 'expr' using nb bins.\\n      The return value is an array of (x,y) pairs representing the centers of the\\n      histogram's bins. As the value of 'nb' is increased, the histogram approximation\\n      gets finer-grained, but may yield artifacts around outliers. In practice, 20-40\\n      histogram bins appear to work well, with more bins being required for skewed or\\n      smaller datasets. Note that this function creates a histogram with non-uniform\\n      bin widths. It offers no guarantees in terms of the mean-squared-error of the\\n      histogram, but in practice is comparable to the histograms produced by the R/S-Plus\\n      statistical computing packages. Note: the output type of the 'x' field in the return value is\\n      propagated from the input value consumed in the aggregate function.\\n    \", className='org.apache.spark.sql.catalyst.expressions.aggregate.HistogramNumeric', isTemporary=True),\n",
       " Function(name='hll_sketch_agg', catalog=None, namespace=None, description=\"\\n    hll_sketch_agg(expr, lgConfigK) - Returns the HllSketch's updatable binary representation.\\n      `lgConfigK` (optional) the log-base-2 of K, with K is the number of buckets or\\n      slots for the HllSketch. \", className='org.apache.spark.sql.catalyst.expressions.aggregate.HllSketchAgg', isTemporary=True),\n",
       " Function(name='hll_sketch_estimate', catalog=None, namespace=None, description='\\n    hll_sketch_estimate(expr) - Returns the estimated number of unique values given the binary representation\\n    of a Datasketches HllSketch. ', className='org.apache.spark.sql.catalyst.expressions.HllSketchEstimate', isTemporary=True),\n",
       " Function(name='hll_union', catalog=None, namespace=None, description='\\n    hll_union(first, second, allowDifferentLgConfigK) - Merges two binary representations of\\n    Datasketches HllSketch objects, using a Datasketches Union object. Set\\n    allowDifferentLgConfigK to true to allow unions of sketches with different\\n    lgConfigK values (defaults to false). ', className='org.apache.spark.sql.catalyst.expressions.HllUnion', isTemporary=True),\n",
       " Function(name='hll_union_agg', catalog=None, namespace=None, description='\\n    hll_union_agg(expr, allowDifferentLgConfigK) - Returns the estimated number of unique values.\\n      `allowDifferentLgConfigK` (optional) Allow sketches with different lgConfigK values\\n       to be unioned (defaults to false).', className='org.apache.spark.sql.catalyst.expressions.aggregate.HllUnionAgg', isTemporary=True),\n",
       " Function(name='hour', catalog=None, namespace=None, description='hour(timestamp) - Returns the hour component of the string/timestamp.', className='org.apache.spark.sql.catalyst.expressions.Hour', isTemporary=True),\n",
       " Function(name='hypot', catalog=None, namespace=None, description='hypot(expr1, expr2) - Returns sqrt(`expr1`**2 + `expr2`**2).', className='org.apache.spark.sql.catalyst.expressions.Hypot', isTemporary=True),\n",
       " Function(name='if', catalog=None, namespace=None, description='if(expr1, expr2, expr3) - If `expr1` evaluates to true, then returns `expr2`; otherwise returns `expr3`.', className='org.apache.spark.sql.catalyst.expressions.If', isTemporary=True),\n",
       " Function(name='ifnull', catalog=None, namespace=None, description='ifnull(expr1, expr2) - Returns `expr2` if `expr1` is null, or `expr1` otherwise.', className='org.apache.spark.sql.catalyst.expressions.Nvl', isTemporary=True),\n",
       " Function(name='ilike', catalog=None, namespace=None, description='str ilike pattern[ ESCAPE escape] - Returns true if str matches `pattern` with `escape` case-insensitively, null if any arguments are null, false otherwise.', className='org.apache.spark.sql.catalyst.expressions.ILike', isTemporary=True),\n",
       " Function(name='in', catalog=None, namespace=None, description='expr1 in(expr2, expr3, ...) - Returns true if `expr` equals to any valN.', className='org.apache.spark.sql.catalyst.expressions.In', isTemporary=True),\n",
       " Function(name='initcap', catalog=None, namespace=None, description='\\n    initcap(str) - Returns `str` with the first letter of each word in uppercase.\\n      All other letters are in lowercase. Words are delimited by white space.\\n  ', className='org.apache.spark.sql.catalyst.expressions.InitCap', isTemporary=True),\n",
       " Function(name='inline', catalog=None, namespace=None, description='inline(expr) - Explodes an array of structs into a table. Uses column names col1, col2, etc. by default unless specified otherwise.', className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True),\n",
       " Function(name='inline_outer', catalog=None, namespace=None, description='inline_outer(expr) - Explodes an array of structs into a table. Uses column names col1, col2, etc. by default unless specified otherwise.', className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True),\n",
       " Function(name='input_file_block_length', catalog=None, namespace=None, description='input_file_block_length() - Returns the length of the block being read, or -1 if not available.', className='org.apache.spark.sql.catalyst.expressions.InputFileBlockLength', isTemporary=True),\n",
       " Function(name='input_file_block_start', catalog=None, namespace=None, description='input_file_block_start() - Returns the start offset of the block being read, or -1 if not available.', className='org.apache.spark.sql.catalyst.expressions.InputFileBlockStart', isTemporary=True),\n",
       " Function(name='input_file_name', catalog=None, namespace=None, description='input_file_name() - Returns the name of the file being read, or empty string if not available.', className='org.apache.spark.sql.catalyst.expressions.InputFileName', isTemporary=True),\n",
       " Function(name='instr', catalog=None, namespace=None, description='instr(str, substr) - Returns the (1-based) index of the first occurrence of `substr` in `str`.', className='org.apache.spark.sql.catalyst.expressions.StringInstr', isTemporary=True),\n",
       " Function(name='int', catalog=None, namespace=None, description='int(expr) - Casts the value `expr` to the target data type `int`.', className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='isnan', catalog=None, namespace=None, description='isnan(expr) - Returns true if `expr` is NaN, or false otherwise.', className='org.apache.spark.sql.catalyst.expressions.IsNaN', isTemporary=True),\n",
       " Function(name='isnotnull', catalog=None, namespace=None, description='isnotnull(expr) - Returns true if `expr` is not null, or false otherwise.', className='org.apache.spark.sql.catalyst.expressions.IsNotNull', isTemporary=True),\n",
       " Function(name='isnull', catalog=None, namespace=None, description='isnull(expr) - Returns true if `expr` is null, or false otherwise.', className='org.apache.spark.sql.catalyst.expressions.IsNull', isTemporary=True),\n",
       " Function(name='java_method', catalog=None, namespace=None, description='java_method(class, method[, arg1[, arg2 ..]]) - Calls a method with reflection.', className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True),\n",
       " Function(name='json_array_length', catalog=None, namespace=None, description='json_array_length(jsonArray) - Returns the number of elements in the outermost JSON array.', className='org.apache.spark.sql.catalyst.expressions.LengthOfJsonArray', isTemporary=True),\n",
       " Function(name='json_object_keys', catalog=None, namespace=None, description='json_object_keys(json_object) - Returns all the keys of the outermost JSON object as an array.', className='org.apache.spark.sql.catalyst.expressions.JsonObjectKeys', isTemporary=True),\n",
       " Function(name='json_tuple', catalog=None, namespace=None, description='json_tuple(jsonStr, p1, p2, ..., pn) - Returns a tuple like the function get_json_object, but it takes multiple names. All the input parameters and output column types are string.', className='org.apache.spark.sql.catalyst.expressions.JsonTuple', isTemporary=True),\n",
       " Function(name='kurtosis', catalog=None, namespace=None, description='kurtosis(expr) - Returns the kurtosis value calculated from values of a group.', className='org.apache.spark.sql.catalyst.expressions.aggregate.Kurtosis', isTemporary=True),\n",
       " Function(name='lag', catalog=None, namespace=None, description='\\n    lag(input[, offset[, default]]) - Returns the value of `input` at the `offset`th row\\n      before the current row in the window. The default value of `offset` is 1 and the default\\n      value of `default` is null. If the value of `input` at the `offset`th row is null,\\n      null is returned. If there is no such offset row (e.g., when the offset is 1, the first\\n      row of the window does not have any previous row), `default` is returned.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Lag', isTemporary=True),\n",
       " Function(name='last', catalog=None, namespace=None, description='\\n    last(expr[, isIgnoreNull]) - Returns the last value of `expr` for a group of rows.\\n      If `isIgnoreNull` is true, returns only non-null values', className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True),\n",
       " Function(name='last_day', catalog=None, namespace=None, description='last_day(date) - Returns the last day of the month which the date belongs to.', className='org.apache.spark.sql.catalyst.expressions.LastDay', isTemporary=True),\n",
       " Function(name='last_value', catalog=None, namespace=None, description='\\n    last_value(expr[, isIgnoreNull]) - Returns the last value of `expr` for a group of rows.\\n      If `isIgnoreNull` is true, returns only non-null values', className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True),\n",
       " Function(name='lcase', catalog=None, namespace=None, description='lcase(str) - Returns `str` with all characters changed to lowercase.', className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True),\n",
       " Function(name='lead', catalog=None, namespace=None, description='\\n    lead(input[, offset[, default]]) - Returns the value of `input` at the `offset`th row\\n      after the current row in the window. The default value of `offset` is 1 and the default\\n      value of `default` is null. If the value of `input` at the `offset`th row is null,\\n      null is returned. If there is no such an offset row (e.g., when the offset is 1, the last\\n      row of the window does not have any subsequent row), `default` is returned.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Lead', isTemporary=True),\n",
       " Function(name='least', catalog=None, namespace=None, description='least(expr, ...) - Returns the least value of all parameters, skipping null values.', className='org.apache.spark.sql.catalyst.expressions.Least', isTemporary=True),\n",
       " Function(name='left', catalog=None, namespace=None, description='left(str, len) - Returns the leftmost `len`(`len` can be string type) characters from the string `str`,if `len` is less or equal than 0 the result is an empty string.', className='org.apache.spark.sql.catalyst.expressions.Left', isTemporary=True),\n",
       " Function(name='len', catalog=None, namespace=None, description='len(expr) - Returns the character length of string data or number of bytes of binary data. The length of string data includes the trailing spaces. The length of binary data includes binary zeros.', className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
       " Function(name='length', catalog=None, namespace=None, description='length(expr) - Returns the character length of string data or number of bytes of binary data. The length of string data includes the trailing spaces. The length of binary data includes binary zeros.', className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
       " Function(name='levenshtein', catalog=None, namespace=None, description='\\n    levenshtein(str1, str2[, threshold]) - Returns the Levenshtein distance between the two given strings. If threshold is set and distance more than it, return -1.', className='org.apache.spark.sql.catalyst.expressions.Levenshtein', isTemporary=True),\n",
       " Function(name='like', catalog=None, namespace=None, description='str like pattern[ ESCAPE escape] - Returns true if str matches `pattern` with `escape`, null if any arguments are null, false otherwise.', className='org.apache.spark.sql.catalyst.expressions.Like', isTemporary=True),\n",
       " Function(name='ln', catalog=None, namespace=None, description='ln(expr) - Returns the natural logarithm (base e) of `expr`.', className='org.apache.spark.sql.catalyst.expressions.Log', isTemporary=True),\n",
       " Function(name='localtimestamp', catalog=None, namespace=None, description='\\n    localtimestamp() - Returns the current timestamp without time zone at the start of query evaluation. All calls of localtimestamp within the same query return the same value.\\n\\n    localtimestamp - Returns the current local date-time at the session time zone at the start of query evaluation.\\n  ', className='org.apache.spark.sql.catalyst.expressions.LocalTimestamp', isTemporary=True),\n",
       " Function(name='locate', catalog=None, namespace=None, description='\\n    locate(substr, str[, pos]) - Returns the position of the first occurrence of `substr` in `str` after position `pos`.\\n      The given `pos` and return value are 1-based.\\n  ', className='org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True),\n",
       " Function(name='log', catalog=None, namespace=None, description='log(base, expr) - Returns the logarithm of `expr` with `base`.', className='org.apache.spark.sql.catalyst.expressions.Logarithm', isTemporary=True),\n",
       " Function(name='log10', catalog=None, namespace=None, description='log10(expr) - Returns the logarithm of `expr` with base 10.', className='org.apache.spark.sql.catalyst.expressions.Log10', isTemporary=True),\n",
       " Function(name='log1p', catalog=None, namespace=None, description='log1p(expr) - Returns log(1 + `expr`).', className='org.apache.spark.sql.catalyst.expressions.Log1p', isTemporary=True),\n",
       " Function(name='log2', catalog=None, namespace=None, description='log2(expr) - Returns the logarithm of `expr` with base 2.', className='org.apache.spark.sql.catalyst.expressions.Log2', isTemporary=True),\n",
       " Function(name='lower', catalog=None, namespace=None, description='lower(str) - Returns `str` with all characters changed to lowercase.', className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True),\n",
       " Function(name='lpad', catalog=None, namespace=None, description='\\n    lpad(str, len[, pad]) - Returns `str`, left-padded with `pad` to a length of `len`.\\n      If `str` is longer than `len`, the return value is shortened to `len` characters or bytes.\\n      If `pad` is not specified, `str` will be padded to the left with space characters if it is\\n      a character string, and with zeros if it is a byte sequence.\\n  ', className='org.apache.spark.sql.catalyst.expressions.LPadExpressionBuilder', isTemporary=True),\n",
       " Function(name='ltrim', catalog=None, namespace=None, description='\\n    ltrim(str) - Removes the leading space characters from `str`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.StringTrimLeft', isTemporary=True),\n",
       " Function(name='luhn_check', catalog=None, namespace=None, description='\\n    luhn_check(str ) - Checks that a string of digits is valid according to the Luhn algorithm.\\n    This checksum function is widely applied on credit card numbers and government identification\\n    numbers to distinguish valid numbers from mistyped, incorrect numbers.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Luhncheck', isTemporary=True),\n",
       " Function(name='make_date', catalog=None, namespace=None, description='make_date(year, month, day) - Create date from year, month and day fields. If the configuration `spark.sql.ansi.enabled` is false, the function returns NULL on invalid inputs. Otherwise, it will throw an error instead.', className='org.apache.spark.sql.catalyst.expressions.MakeDate', isTemporary=True),\n",
       " Function(name='make_dt_interval', catalog=None, namespace=None, description='make_dt_interval([days[, hours[, mins[, secs]]]]) - Make DayTimeIntervalType duration from days, hours, mins and secs.', className='org.apache.spark.sql.catalyst.expressions.MakeDTInterval', isTemporary=True),\n",
       " Function(name='make_interval', catalog=None, namespace=None, description='make_interval([years[, months[, weeks[, days[, hours[, mins[, secs]]]]]]]) - Make interval from years, months, weeks, days, hours, mins and secs.', className='org.apache.spark.sql.catalyst.expressions.MakeInterval', isTemporary=True),\n",
       " Function(name='make_timestamp', catalog=None, namespace=None, description='make_timestamp(year, month, day, hour, min, sec[, timezone]) - Create timestamp from year, month, day, hour, min, sec and timezone fields. The result data type is consistent with the value of configuration `spark.sql.timestampType`. If the configuration `spark.sql.ansi.enabled` is false, the function returns NULL on invalid inputs. Otherwise, it will throw an error instead.', className='org.apache.spark.sql.catalyst.expressions.MakeTimestamp', isTemporary=True),\n",
       " Function(name='make_timestamp_ltz', catalog=None, namespace=None, description='make_timestamp_ltz(year, month, day, hour, min, sec[, timezone]) - Create the current timestamp with local time zone from year, month, day, hour, min, sec and timezone fields. If the configuration `spark.sql.ansi.enabled` is false, the function returns NULL on invalid inputs. Otherwise, it will throw an error instead.', className='org.apache.spark.sql.catalyst.expressions.MakeTimestampLTZExpressionBuilder', isTemporary=True),\n",
       " Function(name='make_timestamp_ntz', catalog=None, namespace=None, description='make_timestamp_ntz(year, month, day, hour, min, sec) - Create local date-time from year, month, day, hour, min, sec fields. If the configuration `spark.sql.ansi.enabled` is false, the function returns NULL on invalid inputs. Otherwise, it will throw an error instead.', className='org.apache.spark.sql.catalyst.expressions.MakeTimestampNTZExpressionBuilder', isTemporary=True),\n",
       " Function(name='make_ym_interval', catalog=None, namespace=None, description='make_ym_interval([years[, months]]) - Make year-month interval from years, months.', className='org.apache.spark.sql.catalyst.expressions.MakeYMInterval', isTemporary=True),\n",
       " Function(name='map', catalog=None, namespace=None, description='map(key0, value0, key1, value1, ...) - Creates a map with the given key/value pairs.', className='org.apache.spark.sql.catalyst.expressions.CreateMap', isTemporary=True),\n",
       " Function(name='map_concat', catalog=None, namespace=None, description='map_concat(map, ...) - Returns the union of all the given maps', className='org.apache.spark.sql.catalyst.expressions.MapConcat', isTemporary=True),\n",
       " Function(name='map_contains_key', catalog=None, namespace=None, description='map_contains_key(map, key) - Returns true if the map contains the key.', className='org.apache.spark.sql.catalyst.expressions.MapContainsKey', isTemporary=True),\n",
       " Function(name='map_entries', catalog=None, namespace=None, description='map_entries(map) - Returns an unordered array of all entries in the given map.', className='org.apache.spark.sql.catalyst.expressions.MapEntries', isTemporary=True),\n",
       " Function(name='map_filter', catalog=None, namespace=None, description='map_filter(expr, func) - Filters entries in a map using the function.', className='org.apache.spark.sql.catalyst.expressions.MapFilter', isTemporary=True),\n",
       " Function(name='map_from_arrays', catalog=None, namespace=None, description='\\n    map_from_arrays(keys, values) - Creates a map with a pair of the given key/value arrays. All elements\\n      in keys should not be null', className='org.apache.spark.sql.catalyst.expressions.MapFromArrays', isTemporary=True),\n",
       " Function(name='map_from_entries', catalog=None, namespace=None, description='map_from_entries(arrayOfEntries) - Returns a map created from the given array of entries.', className='org.apache.spark.sql.catalyst.expressions.MapFromEntries', isTemporary=True),\n",
       " Function(name='map_keys', catalog=None, namespace=None, description='map_keys(map) - Returns an unordered array containing the keys of the map.', className='org.apache.spark.sql.catalyst.expressions.MapKeys', isTemporary=True),\n",
       " Function(name='map_values', catalog=None, namespace=None, description='map_values(map) - Returns an unordered array containing the values of the map.', className='org.apache.spark.sql.catalyst.expressions.MapValues', isTemporary=True),\n",
       " Function(name='map_zip_with', catalog=None, namespace=None, description='\\n      map_zip_with(map1, map2, function) - Merges two given maps into a single map by applying\\n      function to the pair of values with the same key. For keys only presented in one map,\\n      NULL will be passed as the value for the missing key. If an input map contains duplicated\\n      keys, only the first entry of the duplicated key is passed into the lambda function.\\n    ', className='org.apache.spark.sql.catalyst.expressions.MapZipWith', isTemporary=True),\n",
       " Function(name='mask', catalog=None, namespace=None, description=\"mask(input[, upperChar, lowerChar, digitChar, otherChar]) - masks the given string value.\\n       The function replaces characters with 'X' or 'x', and numbers with 'n'.\\n       This can be useful for creating copies of tables with sensitive information removed.\\n      \", className='org.apache.spark.sql.catalyst.expressions.MaskExpressionBuilder', isTemporary=True),\n",
       " Function(name='max', catalog=None, namespace=None, description='max(expr) - Returns the maximum value of `expr`.', className='org.apache.spark.sql.catalyst.expressions.aggregate.Max', isTemporary=True),\n",
       " Function(name='max_by', catalog=None, namespace=None, description='max_by(x, y) - Returns the value of `x` associated with the maximum value of `y`.', className='org.apache.spark.sql.catalyst.expressions.aggregate.MaxBy', isTemporary=True),\n",
       " Function(name='md5', catalog=None, namespace=None, description='md5(expr) - Returns an MD5 128-bit checksum as a hex string of `expr`.', className='org.apache.spark.sql.catalyst.expressions.Md5', isTemporary=True),\n",
       " Function(name='mean', catalog=None, namespace=None, description='mean(expr) - Returns the mean calculated from values of a group.', className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True),\n",
       " Function(name='median', catalog=None, namespace=None, description='median(col) - Returns the median of numeric or ANSI interval column `col`.', className='org.apache.spark.sql.catalyst.expressions.aggregate.Median', isTemporary=True),\n",
       " Function(name='min', catalog=None, namespace=None, description='min(expr) - Returns the minimum value of `expr`.', className='org.apache.spark.sql.catalyst.expressions.aggregate.Min', isTemporary=True),\n",
       " Function(name='min_by', catalog=None, namespace=None, description='min_by(x, y) - Returns the value of `x` associated with the minimum value of `y`.', className='org.apache.spark.sql.catalyst.expressions.aggregate.MinBy', isTemporary=True),\n",
       " Function(name='minute', catalog=None, namespace=None, description='minute(timestamp) - Returns the minute component of the string/timestamp.', className='org.apache.spark.sql.catalyst.expressions.Minute', isTemporary=True),\n",
       " Function(name='mod', catalog=None, namespace=None, description='expr1 mod expr2 - Returns the remainder after `expr1`/`expr2`.', className='org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True),\n",
       " Function(name='mode', catalog=None, namespace=None, description='mode(col) - Returns the most frequent value for the values within `col`. NULL values are ignored. If all the values are NULL, or there are 0 rows, returns NULL.', className='org.apache.spark.sql.catalyst.expressions.aggregate.Mode', isTemporary=True),\n",
       " Function(name='monotonically_increasing_id', catalog=None, namespace=None, description='\\n    monotonically_increasing_id() - Returns monotonically increasing 64-bit integers. The generated ID is guaranteed\\n      to be monotonically increasing and unique, but not consecutive. The current implementation\\n      puts the partition ID in the upper 31 bits, and the lower 33 bits represent the record number\\n      within each partition. The assumption is that the data frame has less than 1 billion\\n      partitions, and each partition has less than 8 billion records.\\n      The function is non-deterministic because its result depends on partition IDs.\\n  ', className='org.apache.spark.sql.catalyst.expressions.MonotonicallyIncreasingID', isTemporary=True),\n",
       " Function(name='month', catalog=None, namespace=None, description='month(date) - Returns the month component of the date/timestamp.', className='org.apache.spark.sql.catalyst.expressions.Month', isTemporary=True),\n",
       " Function(name='months_between', catalog=None, namespace=None, description='\\n    months_between(timestamp1, timestamp2[, roundOff]) - If `timestamp1` is later than `timestamp2`, then the result\\n      is positive. If `timestamp1` and `timestamp2` are on the same day of month, or both\\n      are the last day of month, time of day will be ignored. Otherwise, the difference is\\n      calculated based on 31 days per month, and rounded to 8 digits unless roundOff=false.\\n  ', className='org.apache.spark.sql.catalyst.expressions.MonthsBetween', isTemporary=True),\n",
       " Function(name='named_struct', catalog=None, namespace=None, description='named_struct(name1, val1, name2, val2, ...) - Creates a struct with the given field names and values.', className='org.apache.spark.sql.catalyst.expressions.CreateNamedStruct', isTemporary=True),\n",
       " Function(name='nanvl', catalog=None, namespace=None, description=\"nanvl(expr1, expr2) - Returns `expr1` if it's not NaN, or `expr2` otherwise.\", className='org.apache.spark.sql.catalyst.expressions.NaNvl', isTemporary=True),\n",
       " Function(name='negative', catalog=None, namespace=None, description='negative(expr) - Returns the negated value of `expr`.', className='org.apache.spark.sql.catalyst.expressions.UnaryMinus', isTemporary=True),\n",
       " Function(name='next_day', catalog=None, namespace=None, description='next_day(start_date, day_of_week) - Returns the first date which is later than `start_date` and named as indicated.\\n      The function returns NULL if at least one of the input parameters is NULL.\\n      When both of the input parameters are not NULL and day_of_week is an invalid input,\\n      the function throws IllegalArgumentException if `spark.sql.ansi.enabled` is set to true, otherwise NULL.\\n      ', className='org.apache.spark.sql.catalyst.expressions.NextDay', isTemporary=True),\n",
       " Function(name='not', catalog=None, namespace=None, description='not expr - Logical not.', className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True),\n",
       " Function(name='now', catalog=None, namespace=None, description='now() - Returns the current timestamp at the start of query evaluation.', className='org.apache.spark.sql.catalyst.expressions.Now', isTemporary=True),\n",
       " Function(name='nth_value', catalog=None, namespace=None, description='\\n    nth_value(input[, offset]) - Returns the value of `input` at the row that is the `offset`th row\\n      from beginning of the window frame. Offset starts at 1. If ignoreNulls=true, we will skip\\n      nulls when finding the `offset`th row. Otherwise, every row counts for the `offset`. If\\n      there is no such an `offset`th row (e.g., when the offset is 10, size of the window frame\\n      is less than 10), null is returned.\\n  ', className='org.apache.spark.sql.catalyst.expressions.NthValue', isTemporary=True),\n",
       " Function(name='ntile', catalog=None, namespace=None, description='\\n    ntile(n) - Divides the rows for each window partition into `n` buckets ranging\\n      from 1 to at most `n`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.NTile', isTemporary=True),\n",
       " Function(name='nullif', catalog=None, namespace=None, description='nullif(expr1, expr2) - Returns null if `expr1` equals to `expr2`, or `expr1` otherwise.', className='org.apache.spark.sql.catalyst.expressions.NullIf', isTemporary=True),\n",
       " Function(name='nvl', catalog=None, namespace=None, description='nvl(expr1, expr2) - Returns `expr2` if `expr1` is null, or `expr1` otherwise.', className='org.apache.spark.sql.catalyst.expressions.Nvl', isTemporary=True),\n",
       " Function(name='nvl2', catalog=None, namespace=None, description='nvl2(expr1, expr2, expr3) - Returns `expr2` if `expr1` is not null, or `expr3` otherwise.', className='org.apache.spark.sql.catalyst.expressions.Nvl2', isTemporary=True),\n",
       " Function(name='octet_length', catalog=None, namespace=None, description='octet_length(expr) - Returns the byte length of string data or number of bytes of binary data.', className='org.apache.spark.sql.catalyst.expressions.OctetLength', isTemporary=True),\n",
       " Function(name='or', catalog=None, namespace=None, description='expr1 or expr2 - Logical OR.', className='org.apache.spark.sql.catalyst.expressions.Or', isTemporary=True),\n",
       " Function(name='overlay', catalog=None, namespace=None, description='overlay(input, replace, pos[, len]) - Replace `input` with `replace` that starts at `pos` and is of length `len`.', className='org.apache.spark.sql.catalyst.expressions.Overlay', isTemporary=True),\n",
       " Function(name='parse_url', catalog=None, namespace=None, description='parse_url(url, partToExtract[, key]) - Extracts a part from a URL.', className='org.apache.spark.sql.catalyst.expressions.ParseUrl', isTemporary=True),\n",
       " Function(name='percent_rank', catalog=None, namespace=None, description='\\n    percent_rank() - Computes the percentage ranking of a value in a group of values.\\n  ', className='org.apache.spark.sql.catalyst.expressions.PercentRank', isTemporary=True),\n",
       " Function(name='percentile', catalog=None, namespace=None, description='\\n      percentile(col, percentage [, frequency]) - Returns the exact percentile value of numeric\\n       or ANSI interval column `col` at the given percentage. The value of percentage must be\\n       between 0.0 and 1.0. The value of frequency should be positive integral\\n\\n      percentile(col, array(percentage1 [, percentage2]...) [, frequency]) - Returns the exact\\n      percentile value array of numeric column `col` at the given percentage(s). Each value\\n      of the percentage array must be between 0.0 and 1.0. The value of frequency should be\\n      positive integral\\n\\n      ', className='org.apache.spark.sql.catalyst.expressions.aggregate.Percentile', isTemporary=True),\n",
       " Function(name='percentile_approx', catalog=None, namespace=None, description='\\n    percentile_approx(col, percentage [, accuracy]) - Returns the approximate `percentile` of the numeric or\\n      ansi interval column `col` which is the smallest value in the ordered `col` values (sorted\\n      from least to greatest) such that no more than `percentage` of `col` values is less than\\n      the value or equal to that value. The value of percentage must be between 0.0 and 1.0.\\n      The `accuracy` parameter (default: 10000) is a positive numeric literal which controls\\n      approximation accuracy at the cost of memory. Higher value of `accuracy` yields better\\n      accuracy, `1.0/accuracy` is the relative error of the approximation.\\n      When `percentage` is an array, each value of the percentage array must be between 0.0 and 1.0.\\n      In this case, returns the approximate percentile array of column `col` at the given\\n      percentage array.\\n  ', className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True),\n",
       " Function(name='pi', catalog=None, namespace=None, description='pi() - Returns pi.', className='org.apache.spark.sql.catalyst.expressions.Pi', isTemporary=True),\n",
       " Function(name='pmod', catalog=None, namespace=None, description='pmod(expr1, expr2) - Returns the positive value of `expr1` mod `expr2`.', className='org.apache.spark.sql.catalyst.expressions.Pmod', isTemporary=True),\n",
       " Function(name='posexplode', catalog=None, namespace=None, description='posexplode(expr) - Separates the elements of array `expr` into multiple rows with positions, or the elements of map `expr` into multiple rows and columns with positions. Unless specified otherwise, uses the column name `pos` for position, `col` for elements of the array or `key` and `value` for elements of the map.', className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True),\n",
       " Function(name='posexplode_outer', catalog=None, namespace=None, description='posexplode_outer(expr) - Separates the elements of array `expr` into multiple rows with positions, or the elements of map `expr` into multiple rows and columns with positions. Unless specified otherwise, uses the column name `pos` for position, `col` for elements of the array or `key` and `value` for elements of the map.', className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True),\n",
       " Function(name='position', catalog=None, namespace=None, description='\\n    position(substr, str[, pos]) - Returns the position of the first occurrence of `substr` in `str` after position `pos`.\\n      The given `pos` and return value are 1-based.\\n  ', className='org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True),\n",
       " Function(name='positive', catalog=None, namespace=None, description='positive(expr) - Returns the value of `expr`.', className='org.apache.spark.sql.catalyst.expressions.UnaryPositive', isTemporary=True),\n",
       " Function(name='pow', catalog=None, namespace=None, description='pow(expr1, expr2) - Raises `expr1` to the power of `expr2`.', className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True),\n",
       " Function(name='power', catalog=None, namespace=None, description='power(expr1, expr2) - Raises `expr1` to the power of `expr2`.', className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True),\n",
       " Function(name='printf', catalog=None, namespace=None, description='printf(strfmt, obj, ...) - Returns a formatted string from printf-style format strings.', className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True),\n",
       " Function(name='quarter', catalog=None, namespace=None, description='quarter(date) - Returns the quarter of the year for date, in the range 1 to 4.', className='org.apache.spark.sql.catalyst.expressions.Quarter', isTemporary=True),\n",
       " Function(name='radians', catalog=None, namespace=None, description='radians(expr) - Converts degrees to radians.', className='org.apache.spark.sql.catalyst.expressions.ToRadians', isTemporary=True),\n",
       " Function(name='raise_error', catalog=None, namespace=None, description='raise_error(expr) - Throws an exception with `expr`.', className='org.apache.spark.sql.catalyst.expressions.RaiseError', isTemporary=True),\n",
       " Function(name='rand', catalog=None, namespace=None, description='rand([seed]) - Returns a random value with independent and identically distributed (i.i.d.) uniformly distributed values in [0, 1).', className='org.apache.spark.sql.catalyst.expressions.Rand', isTemporary=True),\n",
       " Function(name='randn', catalog=None, namespace=None, description='randn([seed]) - Returns a random value with independent and identically distributed (i.i.d.) values drawn from the standard normal distribution.', className='org.apache.spark.sql.catalyst.expressions.Randn', isTemporary=True),\n",
       " Function(name='random', catalog=None, namespace=None, description='random([seed]) - Returns a random value with independent and identically distributed (i.i.d.) uniformly distributed values in [0, 1).', className='org.apache.spark.sql.catalyst.expressions.Rand', isTemporary=True),\n",
       " Function(name='range', catalog=None, namespace=None, description='\\n    range(start: long, end: long, step: long, numSlices: integer)\\n    range(start: long, end: long, step: long)\\n    range(start: long, end: long)\\n    range(end: long)', className='org.apache.spark.sql.catalyst.plans.logical.Range', isTemporary=True),\n",
       " Function(name='rank', catalog=None, namespace=None, description='\\n    rank() - Computes the rank of a value in a group of values. The result is one plus the number\\n      of rows preceding or equal to the current row in the ordering of the partition. The values\\n      will produce gaps in the sequence.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Rank', isTemporary=True),\n",
       " Function(name='reduce', catalog=None, namespace=None, description='\\n      reduce(expr, start, merge, finish) - Applies a binary operator to an initial state and all\\n      elements in the array, and reduces this to a single state. The final state is converted\\n      into the final result by applying a finish function.\\n    ', className='org.apache.spark.sql.catalyst.expressions.ArrayAggregate', isTemporary=True),\n",
       " Function(name='reflect', catalog=None, namespace=None, description='reflect(class, method[, arg1[, arg2 ..]]) - Calls a method with reflection.', className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True),\n",
       " Function(name='regexp', catalog=None, namespace=None, description='regexp(str, regexp) - Returns true if `str` matches `regexp`, or false otherwise.', className='org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True),\n",
       " Function(name='regexp_count', catalog=None, namespace=None, description='\\n    regexp_count(str, regexp) - Returns a count of the number of times that the regular expression pattern `regexp` is matched in the string `str`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.RegExpCount', isTemporary=True),\n",
       " Function(name='regexp_extract', catalog=None, namespace=None, description='\\n    regexp_extract(str, regexp[, idx]) - Extract the first string in the `str` that match the `regexp`\\n    expression and corresponding to the regex group index.\\n  ', className='org.apache.spark.sql.catalyst.expressions.RegExpExtract', isTemporary=True),\n",
       " Function(name='regexp_extract_all', catalog=None, namespace=None, description='\\n    regexp_extract_all(str, regexp[, idx]) - Extract all strings in the `str` that match the `regexp`\\n    expression and corresponding to the regex group index.\\n  ', className='org.apache.spark.sql.catalyst.expressions.RegExpExtractAll', isTemporary=True),\n",
       " Function(name='regexp_instr', catalog=None, namespace=None, description='\\n    regexp_instr(str, regexp) - Searches a string for a regular expression and returns an integer that indicates the beginning position of the matched substring. Positions are 1-based, not 0-based. If no match is found, returns 0.\\n  ', className='org.apache.spark.sql.catalyst.expressions.RegExpInStr', isTemporary=True),\n",
       " Function(name='regexp_like', catalog=None, namespace=None, description='regexp_like(str, regexp) - Returns true if `str` matches `regexp`, or false otherwise.', className='org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True),\n",
       " Function(name='regexp_replace', catalog=None, namespace=None, description='regexp_replace(str, regexp, rep[, position]) - Replaces all substrings of `str` that match `regexp` with `rep`.', className='org.apache.spark.sql.catalyst.expressions.RegExpReplace', isTemporary=True),\n",
       " Function(name='regexp_substr', catalog=None, namespace=None, description='\\n    regexp_substr(str, regexp) - Returns the substring that matches the regular expression `regexp` within the string `str`. If the regular expression is not found, the result is null.\\n  ', className='org.apache.spark.sql.catalyst.expressions.RegExpSubStr', isTemporary=True),\n",
       " Function(name='regr_avgx', catalog=None, namespace=None, description='regr_avgx(y, x) - Returns the average of the independent variable for non-null pairs in a group, where `y` is the dependent variable and `x` is the independent variable.', className='org.apache.spark.sql.catalyst.expressions.aggregate.RegrAvgX', isTemporary=True),\n",
       " Function(name='regr_avgy', catalog=None, namespace=None, description='regr_avgy(y, x) - Returns the average of the dependent variable for non-null pairs in a group, where `y` is the dependent variable and `x` is the independent variable.', className='org.apache.spark.sql.catalyst.expressions.aggregate.RegrAvgY', isTemporary=True),\n",
       " Function(name='regr_count', catalog=None, namespace=None, description='regr_count(y, x) - Returns the number of non-null number pairs in a group, where `y` is the dependent variable and `x` is the independent variable.', className='org.apache.spark.sql.catalyst.expressions.aggregate.RegrCount', isTemporary=True),\n",
       " Function(name='regr_intercept', catalog=None, namespace=None, description='regr_intercept(y, x) - Returns the intercept of the univariate linear regression line for non-null pairs in a group, where `y` is the dependent variable and `x` is the independent variable.', className='org.apache.spark.sql.catalyst.expressions.aggregate.RegrIntercept', isTemporary=True),\n",
       " Function(name='regr_r2', catalog=None, namespace=None, description='regr_r2(y, x) - Returns the coefficient of determination for non-null pairs in a group, where `y` is the dependent variable and `x` is the independent variable.', className='org.apache.spark.sql.catalyst.expressions.aggregate.RegrR2', isTemporary=True),\n",
       " Function(name='regr_slope', catalog=None, namespace=None, description='regr_slope(y, x) - Returns the slope of the linear regression line for non-null pairs in a group, where `y` is the dependent variable and `x` is the independent variable.', className='org.apache.spark.sql.catalyst.expressions.aggregate.RegrSlope', isTemporary=True),\n",
       " Function(name='regr_sxx', catalog=None, namespace=None, description='regr_sxx(y, x) - Returns REGR_COUNT(y, x) * VAR_POP(x) for non-null pairs in a group, where `y` is the dependent variable and `x` is the independent variable.', className='org.apache.spark.sql.catalyst.expressions.aggregate.RegrSXX', isTemporary=True),\n",
       " Function(name='regr_sxy', catalog=None, namespace=None, description='regr_sxy(y, x) - Returns REGR_COUNT(y, x) * COVAR_POP(y, x) for non-null pairs in a group, where `y` is the dependent variable and `x` is the independent variable.', className='org.apache.spark.sql.catalyst.expressions.aggregate.RegrSXY', isTemporary=True),\n",
       " Function(name='regr_syy', catalog=None, namespace=None, description='regr_syy(y, x) - Returns REGR_COUNT(y, x) * VAR_POP(y) for non-null pairs in a group, where `y` is the dependent variable and `x` is the independent variable.', className='org.apache.spark.sql.catalyst.expressions.aggregate.RegrSYY', isTemporary=True),\n",
       " Function(name='repeat', catalog=None, namespace=None, description='repeat(str, n) - Returns the string which repeats the given string value n times.', className='org.apache.spark.sql.catalyst.expressions.StringRepeat', isTemporary=True),\n",
       " Function(name='replace', catalog=None, namespace=None, description='replace(str, search[, replace]) - Replaces all occurrences of `search` with `replace`.', className='org.apache.spark.sql.catalyst.expressions.StringReplace', isTemporary=True),\n",
       " Function(name='reverse', catalog=None, namespace=None, description='reverse(array) - Returns a reversed string or an array with reverse order of elements.', className='org.apache.spark.sql.catalyst.expressions.Reverse', isTemporary=True),\n",
       " Function(name='right', catalog=None, namespace=None, description='right(str, len) - Returns the rightmost `len`(`len` can be string type) characters from the string `str`,if `len` is less or equal than 0 the result is an empty string.', className='org.apache.spark.sql.catalyst.expressions.Right', isTemporary=True),\n",
       " Function(name='rint', catalog=None, namespace=None, description='rint(expr) - Returns the double value that is closest in value to the argument and is equal to a mathematical integer.', className='org.apache.spark.sql.catalyst.expressions.Rint', isTemporary=True),\n",
       " Function(name='rlike', catalog=None, namespace=None, description='rlike(str, regexp) - Returns true if `str` matches `regexp`, or false otherwise.', className='org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True),\n",
       " Function(name='round', catalog=None, namespace=None, description='round(expr, d) - Returns `expr` rounded to `d` decimal places using HALF_UP rounding mode.', className='org.apache.spark.sql.catalyst.expressions.Round', isTemporary=True),\n",
       " Function(name='row_number', catalog=None, namespace=None, description='\\n    row_number() - Assigns a unique, sequential number to each row, starting with one,\\n      according to the ordering of rows within the window partition.\\n  ', className='org.apache.spark.sql.catalyst.expressions.RowNumber', isTemporary=True),\n",
       " Function(name='rpad', catalog=None, namespace=None, description='\\n    rpad(str, len[, pad]) - Returns `str`, right-padded with `pad` to a length of `len`.\\n      If `str` is longer than `len`, the return value is shortened to `len` characters.\\n      If `pad` is not specified, `str` will be padded to the right with space characters if it is\\n      a character string, and with zeros if it is a binary string.\\n  ', className='org.apache.spark.sql.catalyst.expressions.RPadExpressionBuilder', isTemporary=True),\n",
       " Function(name='rtrim', catalog=None, namespace=None, description='\\n    rtrim(str) - Removes the trailing space characters from `str`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.StringTrimRight', isTemporary=True),\n",
       " Function(name='schema_of_csv', catalog=None, namespace=None, description='schema_of_csv(csv[, options]) - Returns schema in the DDL format of CSV string.', className='org.apache.spark.sql.catalyst.expressions.SchemaOfCsv', isTemporary=True),\n",
       " Function(name='schema_of_json', catalog=None, namespace=None, description='schema_of_json(json[, options]) - Returns schema in the DDL format of JSON string.', className='org.apache.spark.sql.catalyst.expressions.SchemaOfJson', isTemporary=True),\n",
       " Function(name='sec', catalog=None, namespace=None, description='\\n    sec(expr) - Returns the secant of `expr`, as if computed by `1/java.lang.Math.cos`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Sec', isTemporary=True),\n",
       " Function(name='second', catalog=None, namespace=None, description='second(timestamp) - Returns the second component of the string/timestamp.', className='org.apache.spark.sql.catalyst.expressions.Second', isTemporary=True),\n",
       " Function(name='sentences', catalog=None, namespace=None, description='sentences(str[, lang, country]) - Splits `str` into an array of array of words.', className='org.apache.spark.sql.catalyst.expressions.Sentences', isTemporary=True),\n",
       " Function(name='sequence', catalog=None, namespace=None, description=\"\\n    sequence(start, stop, step) - Generates an array of elements from start to stop (inclusive),\\n      incrementing by step. The type of the returned elements is the same as the type of argument\\n      expressions.\\n\\n      Supported types are: byte, short, integer, long, date, timestamp.\\n\\n      The start and stop expressions must resolve to the same type.\\n      If start and stop expressions resolve to the 'date' or 'timestamp' type\\n      then the step expression must resolve to the 'interval' or 'year-month interval' or\\n      'day-time interval' type, otherwise to the same type as the start and stop expressions.\\n  \", className='org.apache.spark.sql.catalyst.expressions.Sequence', isTemporary=True),\n",
       " Function(name='session_window', catalog=None, namespace=None, description='\\n    session_window(time_column, gap_duration) - Generates session window given a timestamp specifying column and gap duration.\\n      See <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#types-of-time-windows\">\\'Types of time windows\\'</a> in Structured Streaming guide doc for detailed explanation and examples.\\n  ', className='org.apache.spark.sql.catalyst.expressions.SessionWindow', isTemporary=True),\n",
       " Function(name='sha', catalog=None, namespace=None, description='sha(expr) - Returns a sha1 hash value as a hex string of the `expr`.', className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True),\n",
       " Function(name='sha1', catalog=None, namespace=None, description='sha1(expr) - Returns a sha1 hash value as a hex string of the `expr`.', className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True),\n",
       " Function(name='sha2', catalog=None, namespace=None, description='\\n    sha2(expr, bitLength) - Returns a checksum of SHA-2 family as a hex string of `expr`.\\n      SHA-224, SHA-256, SHA-384, and SHA-512 are supported. Bit length of 0 is equivalent to 256.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Sha2', isTemporary=True),\n",
       " Function(name='shiftleft', catalog=None, namespace=None, description='shiftleft(base, expr) - Bitwise left shift.', className='org.apache.spark.sql.catalyst.expressions.ShiftLeft', isTemporary=True),\n",
       " Function(name='shiftright', catalog=None, namespace=None, description='shiftright(base, expr) - Bitwise (signed) right shift.', className='org.apache.spark.sql.catalyst.expressions.ShiftRight', isTemporary=True),\n",
       " Function(name='shiftrightunsigned', catalog=None, namespace=None, description='shiftrightunsigned(base, expr) - Bitwise unsigned right shift.', className='org.apache.spark.sql.catalyst.expressions.ShiftRightUnsigned', isTemporary=True),\n",
       " Function(name='shuffle', catalog=None, namespace=None, description='shuffle(array) - Returns a random permutation of the given array.', className='org.apache.spark.sql.catalyst.expressions.Shuffle', isTemporary=True),\n",
       " Function(name='sign', catalog=None, namespace=None, description='sign(expr) - Returns -1.0, 0.0 or 1.0 as `expr` is negative, 0 or positive.', className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True),\n",
       " Function(name='signum', catalog=None, namespace=None, description='signum(expr) - Returns -1.0, 0.0 or 1.0 as `expr` is negative, 0 or positive.', className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True),\n",
       " Function(name='sin', catalog=None, namespace=None, description='sin(expr) - Returns the sine of `expr`, as if computed by `java.lang.Math.sin`.', className='org.apache.spark.sql.catalyst.expressions.Sin', isTemporary=True),\n",
       " Function(name='sinh', catalog=None, namespace=None, description='\\n    sinh(expr) - Returns hyperbolic sine of `expr`, as if computed by `java.lang.Math.sinh`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Sinh', isTemporary=True),\n",
       " Function(name='size', catalog=None, namespace=None, description='\\n    size(expr) - Returns the size of an array or a map.\\n    The function returns null for null input if spark.sql.legacy.sizeOfNull is set to false or\\n    spark.sql.ansi.enabled is set to true. Otherwise, the function returns -1 for null input.\\n    With the default settings, the function returns -1 for null input.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True),\n",
       " Function(name='skewness', catalog=None, namespace=None, description='skewness(expr) - Returns the skewness value calculated from values of a group.', className='org.apache.spark.sql.catalyst.expressions.aggregate.Skewness', isTemporary=True),\n",
       " Function(name='slice', catalog=None, namespace=None, description='slice(x, start, length) - Subsets array x starting from index start (array indices start at 1, or starting from the end if start is negative) with the specified length.', className='org.apache.spark.sql.catalyst.expressions.Slice', isTemporary=True),\n",
       " Function(name='smallint', catalog=None, namespace=None, description='smallint(expr) - Casts the value `expr` to the target data type `smallint`.', className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='some', catalog=None, namespace=None, description='some(expr) - Returns true if at least one value of `expr` is true.', className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
       " Function(name='sort_array', catalog=None, namespace=None, description='\\n    sort_array(array[, ascendingOrder]) - Sorts the input array in ascending or descending order\\n      according to the natural ordering of the array elements. NaN is greater than any non-NaN\\n      elements for double/float type. Null elements will be placed at the beginning of the returned\\n      array in ascending order or at the end of the returned array in descending order.\\n  ', className='org.apache.spark.sql.catalyst.expressions.SortArray', isTemporary=True),\n",
       " Function(name='soundex', catalog=None, namespace=None, description='soundex(str) - Returns Soundex code of the string.', className='org.apache.spark.sql.catalyst.expressions.SoundEx', isTemporary=True),\n",
       " Function(name='space', catalog=None, namespace=None, description='space(n) - Returns a string consisting of `n` spaces.', className='org.apache.spark.sql.catalyst.expressions.StringSpace', isTemporary=True),\n",
       " Function(name='spark_partition_id', catalog=None, namespace=None, description='spark_partition_id() - Returns the current partition id.', className='org.apache.spark.sql.catalyst.expressions.SparkPartitionID', isTemporary=True),\n",
       " Function(name='split', catalog=None, namespace=None, description='split(str, regex, limit) - Splits `str` around occurrences that match `regex` and returns an array with a length of at most `limit`', className='org.apache.spark.sql.catalyst.expressions.StringSplit', isTemporary=True),\n",
       " Function(name='split_part', catalog=None, namespace=None, description='\\n    split_part(str, delimiter, partNum) - Splits `str` by delimiter and return\\n      requested part of the split (1-based). If any input is null, returns null.\\n      if `partNum` is out of range of split parts, returns empty string. If `partNum` is 0,\\n      throws an error. If `partNum` is negative, the parts are counted backward from the\\n      end of the string. If the `delimiter` is an empty string, the `str` is not split.\\n  ', className='org.apache.spark.sql.catalyst.expressions.SplitPart', isTemporary=True),\n",
       " Function(name='sql_keywords', catalog=None, namespace=None, description='sql_keywords() - Get Spark SQL keywords', className='org.apache.spark.sql.catalyst.expressions.SQLKeywords', isTemporary=True),\n",
       " Function(name='sqrt', catalog=None, namespace=None, description='sqrt(expr) - Returns the square root of `expr`.', className='org.apache.spark.sql.catalyst.expressions.Sqrt', isTemporary=True),\n",
       " Function(name='stack', catalog=None, namespace=None, description='stack(n, expr1, ..., exprk) - Separates `expr1`, ..., `exprk` into `n` rows. Uses column names col0, col1, etc. by default unless specified otherwise.', className='org.apache.spark.sql.catalyst.expressions.Stack', isTemporary=True),\n",
       " Function(name='startswith', catalog=None, namespace=None, description='\\n    startswith(left, right) - Returns a boolean. The value is True if left starts with right.\\n    Returns NULL if either input expression is NULL. Otherwise, returns False.\\n    Both left or right must be of STRING or BINARY type.\\n  ', className='org.apache.spark.sql.catalyst.expressions.StartsWithExpressionBuilder', isTemporary=True),\n",
       " Function(name='std', catalog=None, namespace=None, description='std(expr) - Returns the sample standard deviation calculated from values of a group.', className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
       " Function(name='stddev', catalog=None, namespace=None, description='stddev(expr) - Returns the sample standard deviation calculated from values of a group.', className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
       " Function(name='stddev_pop', catalog=None, namespace=None, description='stddev_pop(expr) - Returns the population standard deviation calculated from values of a group.', className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevPop', isTemporary=True),\n",
       " Function(name='stddev_samp', catalog=None, namespace=None, description='stddev_samp(expr) - Returns the sample standard deviation calculated from values of a group.', className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
       " Function(name='str_to_map', catalog=None, namespace=None, description=\"str_to_map(text[, pairDelim[, keyValueDelim]]) - Creates a map after splitting the text into key/value pairs using delimiters. Default delimiters are ',' for `pairDelim` and ':' for `keyValueDelim`. Both `pairDelim` and `keyValueDelim` are treated as regular expressions.\", className='org.apache.spark.sql.catalyst.expressions.StringToMap', isTemporary=True),\n",
       " Function(name='string', catalog=None, namespace=None, description='string(expr) - Casts the value `expr` to the target data type `string`.', className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='struct', catalog=None, namespace=None, description='struct(col1, col2, col3, ...) - Creates a struct with the given field values.', className='org.apache.spark.sql.catalyst.expressions.CreateNamedStruct', isTemporary=True),\n",
       " Function(name='substr', catalog=None, namespace=None, description='\\n    substr(str, pos[, len]) - Returns the substring of `str` that starts at `pos` and is of length `len`, or the slice of byte array that starts at `pos` and is of length `len`.\\n\\n    substr(str FROM pos[ FOR len]]) - Returns the substring of `str` that starts at `pos` and is of length `len`, or the slice of byte array that starts at `pos` and is of length `len`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True),\n",
       " Function(name='substring', catalog=None, namespace=None, description='\\n    substring(str, pos[, len]) - Returns the substring of `str` that starts at `pos` and is of length `len`, or the slice of byte array that starts at `pos` and is of length `len`.\\n\\n    substring(str FROM pos[ FOR len]]) - Returns the substring of `str` that starts at `pos` and is of length `len`, or the slice of byte array that starts at `pos` and is of length `len`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True),\n",
       " Function(name='substring_index', catalog=None, namespace=None, description='\\n    substring_index(str, delim, count) - Returns the substring from `str` before `count` occurrences of the delimiter `delim`.\\n      If `count` is positive, everything to the left of the final delimiter (counting from the\\n      left) is returned. If `count` is negative, everything to the right of the final delimiter\\n      (counting from the right) is returned. The function substring_index performs a case-sensitive match\\n      when searching for `delim`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.SubstringIndex', isTemporary=True),\n",
       " Function(name='sum', catalog=None, namespace=None, description='sum(expr) - Returns the sum calculated from values of a group.', className='org.apache.spark.sql.catalyst.expressions.aggregate.Sum', isTemporary=True),\n",
       " Function(name='tan', catalog=None, namespace=None, description='\\n    tan(expr) - Returns the tangent of `expr`, as if computed by `java.lang.Math.tan`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Tan', isTemporary=True),\n",
       " Function(name='tanh', catalog=None, namespace=None, description='\\n    tanh(expr) - Returns the hyperbolic tangent of `expr`, as if computed by\\n      `java.lang.Math.tanh`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.Tanh', isTemporary=True),\n",
       " Function(name='timestamp', catalog=None, namespace=None, description='timestamp(expr) - Casts the value `expr` to the target data type `timestamp`.', className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='timestamp_micros', catalog=None, namespace=None, description='timestamp_micros(microseconds) - Creates timestamp from the number of microseconds since UTC epoch.', className='org.apache.spark.sql.catalyst.expressions.MicrosToTimestamp', isTemporary=True),\n",
       " Function(name='timestamp_millis', catalog=None, namespace=None, description='timestamp_millis(milliseconds) - Creates timestamp from the number of milliseconds since UTC epoch.', className='org.apache.spark.sql.catalyst.expressions.MillisToTimestamp', isTemporary=True),\n",
       " Function(name='timestamp_seconds', catalog=None, namespace=None, description='timestamp_seconds(seconds) - Creates timestamp from the number of seconds (can be fractional) since UTC epoch.', className='org.apache.spark.sql.catalyst.expressions.SecondsToTimestamp', isTemporary=True),\n",
       " Function(name='tinyint', catalog=None, namespace=None, description='tinyint(expr) - Casts the value `expr` to the target data type `tinyint`.', className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='to_binary', catalog=None, namespace=None, description='\\n    to_binary(str[, fmt]) - Converts the input `str` to a binary value based on the supplied `fmt`.\\n      `fmt` can be a case-insensitive string literal of \"hex\", \"utf-8\", \"utf8\", or \"base64\".\\n      By default, the binary format for conversion is \"hex\" if `fmt` is omitted.\\n      The function returns NULL if at least one of the input parameters is NULL.\\n  ', className='org.apache.spark.sql.catalyst.expressions.ToBinary', isTemporary=True),\n",
       " Function(name='to_char', catalog=None, namespace=None, description=\"\\n    to_char(numberExpr, formatExpr) - Convert `numberExpr` to a string based on the `formatExpr`.\\n      Throws an exception if the conversion fails. The format can consist of the following\\n      characters, case insensitive:\\n        '0' or '9': Specifies an expected digit between 0 and 9. A sequence of 0 or 9 in the format\\n          string matches a sequence of digits in the input value, generating a result string of the\\n          same length as the corresponding sequence in the format string. The result string is\\n          left-padded with zeros if the 0/9 sequence comprises more digits than the matching part of\\n          the decimal value, starts with 0, and is before the decimal point. Otherwise, it is\\n          padded with spaces.\\n        '.' or 'D': Specifies the position of the decimal point (optional, only allowed once).\\n        ',' or 'G': Specifies the position of the grouping (thousands) separator (,). There must be\\n          a 0 or 9 to the left and right of each grouping separator.\\n        '$': Specifies the location of the $ currency sign. This character may only be specified\\n          once.\\n        'S' or 'MI': Specifies the position of a '-' or '+' sign (optional, only allowed once at\\n          the beginning or end of the format string). Note that 'S' prints '+' for positive values\\n          but 'MI' prints a space.\\n        'PR': Only allowed at the end of the format string; specifies that the result string will be\\n          wrapped by angle brackets if the input value is negative.\\n          ('<1>').\\n  \", className='org.apache.spark.sql.catalyst.expressions.ToCharacter', isTemporary=True),\n",
       " Function(name='to_csv', catalog=None, namespace=None, description='to_csv(expr[, options]) - Returns a CSV string with a given struct value', className='org.apache.spark.sql.catalyst.expressions.StructsToCsv', isTemporary=True),\n",
       " Function(name='to_date', catalog=None, namespace=None, description='\\n    to_date(date_str[, fmt]) - Parses the `date_str` expression with the `fmt` expression to\\n      a date. Returns null with invalid input. By default, it follows casting rules to a date if\\n      the `fmt` is omitted.\\n  ', className='org.apache.spark.sql.catalyst.expressions.ParseToDate', isTemporary=True),\n",
       " Function(name='to_json', catalog=None, namespace=None, description='to_json(expr[, options]) - Returns a JSON string with a given struct value', className='org.apache.spark.sql.catalyst.expressions.StructsToJson', isTemporary=True),\n",
       " Function(name='to_number', catalog=None, namespace=None, description=\"\\n     to_number(expr, fmt) - Convert string 'expr' to a number based on the string format 'fmt'.\\n       Throws an exception if the conversion fails. The format can consist of the following\\n       characters, case insensitive:\\n         '0' or '9': Specifies an expected digit between 0 and 9. A sequence of 0 or 9 in the format\\n           string matches a sequence of digits in the input string. If the 0/9 sequence starts with\\n           0 and is before the decimal point, it can only match a digit sequence of the same size.\\n           Otherwise, if the sequence starts with 9 or is after the decimal point, it can match a\\n           digit sequence that has the same or smaller size.\\n         '.' or 'D': Specifies the position of the decimal point (optional, only allowed once).\\n         ',' or 'G': Specifies the position of the grouping (thousands) separator (,). There must be\\n           a 0 or 9 to the left and right of each grouping separator. 'expr' must match the\\n           grouping separator relevant for the size of the number.\\n         '$': Specifies the location of the $ currency sign. This character may only be specified\\n           once.\\n         'S' or 'MI': Specifies the position of a '-' or '+' sign (optional, only allowed once at\\n           the beginning or end of the format string). Note that 'S' allows '-' but 'MI' does not.\\n         'PR': Only allowed at the end of the format string; specifies that 'expr' indicates a\\n           negative number with wrapping angled brackets.\\n           ('<1>').\\n  \", className='org.apache.spark.sql.catalyst.expressions.ToNumber', isTemporary=True),\n",
       " Function(name='to_timestamp', catalog=None, namespace=None, description='\\n    to_timestamp(timestamp_str[, fmt]) - Parses the `timestamp_str` expression with the `fmt` expression\\n      to a timestamp. Returns null with invalid input. By default, it follows casting rules to\\n      a timestamp if the `fmt` is omitted. The result data type is consistent with the value of\\n      configuration `spark.sql.timestampType`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.ParseToTimestamp', isTemporary=True),\n",
       " Function(name='to_timestamp_ltz', catalog=None, namespace=None, description='\\n    to_timestamp_ltz(timestamp_str[, fmt]) - Parses the `timestamp_str` expression with the `fmt` expression\\n      to a timestamp with local time zone. Returns null with invalid input. By default, it follows casting rules to\\n      a timestamp if the `fmt` is omitted.\\n  ', className='org.apache.spark.sql.catalyst.expressions.ParseToTimestampLTZExpressionBuilder', isTemporary=True),\n",
       " Function(name='to_timestamp_ntz', catalog=None, namespace=None, description='\\n    to_timestamp_ntz(timestamp_str[, fmt]) - Parses the `timestamp_str` expression with the `fmt` expression\\n      to a timestamp without time zone. Returns null with invalid input. By default, it follows casting rules to\\n      a timestamp if the `fmt` is omitted.\\n  ', className='org.apache.spark.sql.catalyst.expressions.ParseToTimestampNTZExpressionBuilder', isTemporary=True),\n",
       " Function(name='to_unix_timestamp', catalog=None, namespace=None, description='to_unix_timestamp(timeExp[, fmt]) - Returns the UNIX timestamp of the given time.', className='org.apache.spark.sql.catalyst.expressions.ToUnixTimestamp', isTemporary=True),\n",
       " Function(name='to_utc_timestamp', catalog=None, namespace=None, description=\"to_utc_timestamp(timestamp, timezone) - Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in the given time zone, and renders that time as a timestamp in UTC. For example, 'GMT+1' would yield '2017-07-14 01:40:00.0'.\", className='org.apache.spark.sql.catalyst.expressions.ToUTCTimestamp', isTemporary=True),\n",
       " Function(name='to_varchar', catalog=None, namespace=None, description=\"\\n    to_varchar(numberExpr, formatExpr) - Convert `numberExpr` to a string based on the `formatExpr`.\\n      Throws an exception if the conversion fails. The format can consist of the following\\n      characters, case insensitive:\\n        '0' or '9': Specifies an expected digit between 0 and 9. A sequence of 0 or 9 in the format\\n          string matches a sequence of digits in the input value, generating a result string of the\\n          same length as the corresponding sequence in the format string. The result string is\\n          left-padded with zeros if the 0/9 sequence comprises more digits than the matching part of\\n          the decimal value, starts with 0, and is before the decimal point. Otherwise, it is\\n          padded with spaces.\\n        '.' or 'D': Specifies the position of the decimal point (optional, only allowed once).\\n        ',' or 'G': Specifies the position of the grouping (thousands) separator (,). There must be\\n          a 0 or 9 to the left and right of each grouping separator.\\n        '$': Specifies the location of the $ currency sign. This character may only be specified\\n          once.\\n        'S' or 'MI': Specifies the position of a '-' or '+' sign (optional, only allowed once at\\n          the beginning or end of the format string). Note that 'S' prints '+' for positive values\\n          but 'MI' prints a space.\\n        'PR': Only allowed at the end of the format string; specifies that the result string will be\\n          wrapped by angle brackets if the input value is negative.\\n          ('<1>').\\n  \", className='org.apache.spark.sql.catalyst.expressions.ToCharacter', isTemporary=True),\n",
       " Function(name='transform', catalog=None, namespace=None, description='transform(expr, func) - Transforms elements in an array using the function.', className='org.apache.spark.sql.catalyst.expressions.ArrayTransform', isTemporary=True),\n",
       " Function(name='transform_keys', catalog=None, namespace=None, description='transform_keys(expr, func) - Transforms elements in a map using the function.', className='org.apache.spark.sql.catalyst.expressions.TransformKeys', isTemporary=True),\n",
       " Function(name='transform_values', catalog=None, namespace=None, description='transform_values(expr, func) - Transforms values in the map using the function.', className='org.apache.spark.sql.catalyst.expressions.TransformValues', isTemporary=True),\n",
       " Function(name='translate', catalog=None, namespace=None, description='translate(input, from, to) - Translates the `input` string by replacing the characters present in the `from` string with the corresponding characters in the `to` string.', className='org.apache.spark.sql.catalyst.expressions.StringTranslate', isTemporary=True),\n",
       " Function(name='trim', catalog=None, namespace=None, description='\\n    trim(str) - Removes the leading and trailing space characters from `str`.\\n\\n    trim(BOTH FROM str) - Removes the leading and trailing space characters from `str`.\\n\\n    trim(LEADING FROM str) - Removes the leading space characters from `str`.\\n\\n    trim(TRAILING FROM str) - Removes the trailing space characters from `str`.\\n\\n    trim(trimStr FROM str) - Remove the leading and trailing `trimStr` characters from `str`.\\n\\n    trim(BOTH trimStr FROM str) - Remove the leading and trailing `trimStr` characters from `str`.\\n\\n    trim(LEADING trimStr FROM str) - Remove the leading `trimStr` characters from `str`.\\n\\n    trim(TRAILING trimStr FROM str) - Remove the trailing `trimStr` characters from `str`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.StringTrim', isTemporary=True),\n",
       " Function(name='trunc', catalog=None, namespace=None, description='\\n    trunc(date, fmt) - Returns `date` with the time portion of the day truncated to the unit specified by the format model `fmt`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.TruncDate', isTemporary=True),\n",
       " Function(name='try_add', catalog=None, namespace=None, description='try_add(expr1, expr2) - Returns the sum of `expr1`and `expr2` and the result is null on overflow. The acceptable input types are the same with the `+` operator.', className='org.apache.spark.sql.catalyst.expressions.TryAdd', isTemporary=True),\n",
       " Function(name='try_aes_decrypt', catalog=None, namespace=None, description='try_aes_decrypt(expr, key[, mode[, padding[, aad]]]) - This is a special version of `aes_decrypt` that performs the same operation, but returns a NULL value instead of raising an error if the decryption cannot be performed.', className='org.apache.spark.sql.catalyst.expressions.TryAesDecrypt', isTemporary=True),\n",
       " Function(name='try_avg', catalog=None, namespace=None, description='try_avg(expr) - Returns the mean calculated from values of a group and the result is null on overflow.', className='org.apache.spark.sql.catalyst.expressions.aggregate.TryAverageExpressionBuilder', isTemporary=True),\n",
       " Function(name='try_divide', catalog=None, namespace=None, description='try_divide(dividend, divisor) - Returns `dividend`/`divisor`. It always performs floating point division. Its result is always null if `expr2` is 0. `dividend` must be a numeric or an interval. `divisor` must be a numeric.', className='org.apache.spark.sql.catalyst.expressions.TryDivide', isTemporary=True),\n",
       " Function(name='try_element_at', catalog=None, namespace=None, description='\\n    try_element_at(array, index) - Returns element of array at given (1-based) index. If Index is 0,\\n      Spark will throw an error. If index < 0, accesses elements from the last to the first.\\n      The function always returns NULL if the index exceeds the length of the array.\\n\\n    try_element_at(map, key) - Returns value for given key. The function always returns NULL\\n      if the key is not contained in the map.\\n  ', className='org.apache.spark.sql.catalyst.expressions.TryElementAt', isTemporary=True),\n",
       " Function(name='try_multiply', catalog=None, namespace=None, description='try_multiply(expr1, expr2) - Returns `expr1`*`expr2` and the result is null on overflow. The acceptable input types are the same with the `*` operator.', className='org.apache.spark.sql.catalyst.expressions.TryMultiply', isTemporary=True),\n",
       " Function(name='try_subtract', catalog=None, namespace=None, description='try_subtract(expr1, expr2) - Returns `expr1`-`expr2` and the result is null on overflow. The acceptable input types are the same with the `-` operator.', className='org.apache.spark.sql.catalyst.expressions.TrySubtract', isTemporary=True),\n",
       " Function(name='try_sum', catalog=None, namespace=None, description='try_sum(expr) - Returns the sum calculated from values of a group and the result is null on overflow.', className='org.apache.spark.sql.catalyst.expressions.aggregate.TrySumExpressionBuilder', isTemporary=True),\n",
       " Function(name='try_to_binary', catalog=None, namespace=None, description='try_to_binary(str[, fmt]) - This is a special version of `to_binary` that performs the same operation, but returns a NULL value instead of raising an error if the conversion cannot be performed.', className='org.apache.spark.sql.catalyst.expressions.TryToBinary', isTemporary=True),\n",
       " Function(name='try_to_number', catalog=None, namespace=None, description=\"\\n     try_to_number(expr, fmt) - Convert string 'expr' to a number based on the string format `fmt`.\\n       Returns NULL if the string 'expr' does not match the expected format. The format follows the\\n       same semantics as the to_number function.\\n  \", className='org.apache.spark.sql.catalyst.expressions.TryToNumber', isTemporary=True),\n",
       " Function(name='try_to_timestamp', catalog=None, namespace=None, description='\\n    try_to_timestamp(timestamp_str[, fmt]) - Parses the `timestamp_str` expression with the `fmt` expression\\n      to a timestamp. The function always returns null on an invalid input with/without ANSI SQL\\n      mode enabled. By default, it follows casting rules to a timestamp if the `fmt` is omitted.\\n      The result data type is consistent with the value of configuration `spark.sql.timestampType`.\\n  ', className='org.apache.spark.sql.catalyst.expressions.TryToTimestampExpressionBuilder', isTemporary=True),\n",
       " Function(name='typeof', catalog=None, namespace=None, description='typeof(expr) - Return DDL-formatted type string for the data type of the input.', className='org.apache.spark.sql.catalyst.expressions.TypeOf', isTemporary=True),\n",
       " Function(name='ucase', catalog=None, namespace=None, description='ucase(str) - Returns `str` with all characters changed to uppercase.', className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True),\n",
       " Function(name='unbase64', catalog=None, namespace=None, description='unbase64(str) - Converts the argument from a base 64 string `str` to a binary.', className='org.apache.spark.sql.catalyst.expressions.UnBase64', isTemporary=True),\n",
       " Function(name='unhex', catalog=None, namespace=None, description='unhex(expr) - Converts hexadecimal `expr` to binary.', className='org.apache.spark.sql.catalyst.expressions.Unhex', isTemporary=True),\n",
       " Function(name='unix_date', catalog=None, namespace=None, description='unix_date(date) - Returns the number of days since 1970-01-01.', className='org.apache.spark.sql.catalyst.expressions.UnixDate', isTemporary=True),\n",
       " Function(name='unix_micros', catalog=None, namespace=None, description='unix_micros(timestamp) - Returns the number of microseconds since 1970-01-01 00:00:00 UTC.', className='org.apache.spark.sql.catalyst.expressions.UnixMicros', isTemporary=True),\n",
       " Function(name='unix_millis', catalog=None, namespace=None, description='unix_millis(timestamp) - Returns the number of milliseconds since 1970-01-01 00:00:00 UTC. Truncates higher levels of precision.', className='org.apache.spark.sql.catalyst.expressions.UnixMillis', isTemporary=True),\n",
       " Function(name='unix_seconds', catalog=None, namespace=None, description='unix_seconds(timestamp) - Returns the number of seconds since 1970-01-01 00:00:00 UTC. Truncates higher levels of precision.', className='org.apache.spark.sql.catalyst.expressions.UnixSeconds', isTemporary=True),\n",
       " Function(name='unix_timestamp', catalog=None, namespace=None, description='unix_timestamp([timeExp[, fmt]]) - Returns the UNIX timestamp of current or specified time.', className='org.apache.spark.sql.catalyst.expressions.UnixTimestamp', isTemporary=True),\n",
       " Function(name='upper', catalog=None, namespace=None, description='upper(str) - Returns `str` with all characters changed to uppercase.', className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True),\n",
       " Function(name='url_decode', catalog=None, namespace=None, description=\"\\n    url_decode(str) - Decodes a `str` in 'application/x-www-form-urlencoded' format using a specific encoding scheme.\\n  \", className='org.apache.spark.sql.catalyst.expressions.UrlDecode', isTemporary=True),\n",
       " Function(name='url_encode', catalog=None, namespace=None, description=\"\\n    url_encode(str) - Translates a string into 'application/x-www-form-urlencoded' format using a specific encoding scheme.\\n  \", className='org.apache.spark.sql.catalyst.expressions.UrlEncode', isTemporary=True),\n",
       " Function(name='user', catalog=None, namespace=None, description='user() - user name of current execution context.', className='org.apache.spark.sql.catalyst.expressions.CurrentUser', isTemporary=True),\n",
       " Function(name='uuid', catalog=None, namespace=None, description='uuid() - Returns an universally unique identifier (UUID) string. The value is returned as a canonical UUID 36-character string.', className='org.apache.spark.sql.catalyst.expressions.Uuid', isTemporary=True),\n",
       " Function(name='var_pop', catalog=None, namespace=None, description='var_pop(expr) - Returns the population variance calculated from values of a group.', className='org.apache.spark.sql.catalyst.expressions.aggregate.VariancePop', isTemporary=True),\n",
       " Function(name='var_samp', catalog=None, namespace=None, description='var_samp(expr) - Returns the sample variance calculated from values of a group.', className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True),\n",
       " Function(name='variance', catalog=None, namespace=None, description='variance(expr) - Returns the sample variance calculated from values of a group.', className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True),\n",
       " Function(name='version', catalog=None, namespace=None, description='version() - Returns the Spark version. The string contains 2 fields, the first being a release version and the second being a git revision.', className='org.apache.spark.sql.catalyst.expressions.SparkVersion', isTemporary=True),\n",
       " Function(name='weekday', catalog=None, namespace=None, description='weekday(date) - Returns the day of the week for date/timestamp (0 = Monday, 1 = Tuesday, ..., 6 = Sunday).', className='org.apache.spark.sql.catalyst.expressions.WeekDay', isTemporary=True),\n",
       " Function(name='weekofyear', catalog=None, namespace=None, description='weekofyear(date) - Returns the week of the year of the given date. A week is considered to start on a Monday and week 1 is the first week with >3 days.', className='org.apache.spark.sql.catalyst.expressions.WeekOfYear', isTemporary=True),\n",
       " Function(name='when', catalog=None, namespace=None, description='CASE WHEN expr1 THEN expr2 [WHEN expr3 THEN expr4]* [ELSE expr5] END - When `expr1` = true, returns `expr2`; else when `expr3` = true, returns `expr4`; else returns `expr5`.', className='org.apache.spark.sql.catalyst.expressions.CaseWhen', isTemporary=True),\n",
       " Function(name='width_bucket', catalog=None, namespace=None, description='\\n    width_bucket(value, min_value, max_value, num_bucket) - Returns the bucket number to which\\n      `value` would be assigned in an equiwidth histogram with `num_bucket` buckets,\\n      in the range `min_value` to `max_value`.\"\\n  ', className='org.apache.spark.sql.catalyst.expressions.WidthBucket', isTemporary=True),\n",
       " Function(name='window', catalog=None, namespace=None, description='\\n    window(time_column, window_duration[, slide_duration[, start_time]]) - Bucketize rows into one or more time windows given a timestamp specifying column.\\n      Window starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window [12:05,12:10) but not in [12:00,12:05).\\n      Windows can support microsecond precision. Windows in the order of months are not supported.\\n      See <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#window-operations-on-event-time\">\\'Window Operations on Event Time\\'</a> in Structured Streaming guide doc for detailed explanation and examples.\\n  ', className='org.apache.spark.sql.catalyst.expressions.TimeWindow', isTemporary=True),\n",
       " Function(name='window_time', catalog=None, namespace=None, description='\\n    window_time(window_column) - Extract the time value from time/session window column which can be used for event time value of window.\\n      The extracted time is (window.end - 1) which reflects the fact that the the aggregating\\n      windows have exclusive upper bound - [start, end)\\n      See <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#window-operations-on-event-time\">\\'Window Operations on Event Time\\'</a> in Structured Streaming guide doc for detailed explanation and examples.\\n  ', className='org.apache.spark.sql.catalyst.expressions.WindowTime', isTemporary=True),\n",
       " Function(name='xpath', catalog=None, namespace=None, description='xpath(xml, xpath) - Returns a string array of values within the nodes of xml that match the XPath expression.', className='org.apache.spark.sql.catalyst.expressions.xml.XPathList', isTemporary=True),\n",
       " Function(name='xpath_boolean', catalog=None, namespace=None, description='xpath_boolean(xml, xpath) - Returns true if the XPath expression evaluates to true, or if a matching node is found.', className='org.apache.spark.sql.catalyst.expressions.xml.XPathBoolean', isTemporary=True),\n",
       " Function(name='xpath_double', catalog=None, namespace=None, description='xpath_double(xml, xpath) - Returns a double value, the value zero if no match is found, or NaN if a match is found but the value is non-numeric.', className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True),\n",
       " Function(name='xpath_float', catalog=None, namespace=None, description='xpath_float(xml, xpath) - Returns a float value, the value zero if no match is found, or NaN if a match is found but the value is non-numeric.', className='org.apache.spark.sql.catalyst.expressions.xml.XPathFloat', isTemporary=True),\n",
       " Function(name='xpath_int', catalog=None, namespace=None, description='xpath_int(xml, xpath) - Returns an integer value, or the value zero if no match is found, or a match is found but the value is non-numeric.', className='org.apache.spark.sql.catalyst.expressions.xml.XPathInt', isTemporary=True),\n",
       " Function(name='xpath_long', catalog=None, namespace=None, description='xpath_long(xml, xpath) - Returns a long integer value, or the value zero if no match is found, or a match is found but the value is non-numeric.', className='org.apache.spark.sql.catalyst.expressions.xml.XPathLong', isTemporary=True),\n",
       " Function(name='xpath_number', catalog=None, namespace=None, description='xpath_number(xml, xpath) - Returns a double value, the value zero if no match is found, or NaN if a match is found but the value is non-numeric.', className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True),\n",
       " Function(name='xpath_short', catalog=None, namespace=None, description='xpath_short(xml, xpath) - Returns a short integer value, or the value zero if no match is found, or a match is found but the value is non-numeric.', className='org.apache.spark.sql.catalyst.expressions.xml.XPathShort', isTemporary=True),\n",
       " Function(name='xpath_string', catalog=None, namespace=None, description='xpath_string(xml, xpath) - Returns the text contents of the first xml node that matches the XPath expression.', className='org.apache.spark.sql.catalyst.expressions.xml.XPathString', isTemporary=True),\n",
       " Function(name='xxhash64', catalog=None, namespace=None, description='xxhash64(expr1, expr2, ...) - Returns a 64-bit hash value of the arguments. Hash seed is 42.', className='org.apache.spark.sql.catalyst.expressions.XxHash64', isTemporary=True),\n",
       " Function(name='year', catalog=None, namespace=None, description='year(date) - Returns the year component of the date/timestamp.', className='org.apache.spark.sql.catalyst.expressions.Year', isTemporary=True),\n",
       " Function(name='zip_with', catalog=None, namespace=None, description='zip_with(left, right, func) - Merges the two given arrays, element-wise, into a single array using function. If one array is shorter, nulls are appended at the end to match the length of the longer array, before applying function.', className='org.apache.spark.sql.catalyst.expressions.ZipWith', isTemporary=True),\n",
       " Function(name='|', catalog=None, namespace=None, description='expr1 | expr2 - Returns the result of bitwise OR of `expr1` and `expr2`.', className='org.apache.spark.sql.catalyst.expressions.BitwiseOr', isTemporary=True),\n",
       " Function(name='||', catalog=None, namespace=None, description='expr1 || expr2 - Returns the concatenation of `expr1` and `expr2`.', className=None, isTemporary=True),\n",
       " Function(name='~', catalog=None, namespace=None, description='~ expr - Returns the result of bitwise NOT of `expr`.', className='org.apache.spark.sql.catalyst.expressions.BitwiseNot', isTemporary=True)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listFunctions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **currentDatabase()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **setCurrentDatabase()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(\"company\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **cacheTable()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.cacheTable(\"default.hive_emp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **isCached()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.isCached(\"default.hive_emp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **uncacheTable()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.uncacheTable(\"default.hive_emp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify if the table is uncached\n",
    "spark.catalog.isCached(\"default.hive_emp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear cache\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
